<!DOCTYPE html>
<html><head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# article: http://ogp.me/ns/article#"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Machine Learning c&#417; b&#7843;n</title><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"><script src="js/3.1.1-jquery.min.js"></script><script src="js/js-bootstrap.min.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300" rel="stylesheet"><!-- <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet"> --><link href="https://fonts.googleapis.com/css?family=Roboto%7CSource+Sans+Pro" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Ubuntu" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet"><!-- Include CSS SCSS --><link rel="stylesheet" type="text/css" href="css/style-post.css"><link rel="stylesheet" type="text/css" href="css/css-monokai.css"><link rel="stylesheet" type="text/css" href="css/css-mystyle.css"><!-- <link rel="stylesheet" type="text/css" href="/css/github.css" /> --><title>B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)</title><!-- <script>
var pageProperties = {
    
    category: "Optimization",
    
    url: "/2017/01/16/gradientdescent2/",
    title: "B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)",
    scripts: [
        
    ],
};

</script>
<script src="/scripts/modules.js" async></script>
 --><link rel="icon" type="image/png" href="favicons/latex-new_logo9.png" sizes="32x32"><link rel="canonical" href="https://machinelearningcoban.com/2017/01/16/gradientdescent2/"><meta name="author" content="Tiep Vu "><meta property="og:title" content="B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)"><meta property="og:site_name" content="Tiep Vu's blog"><meta property="og:url" content="https://machinelearningcoban.com/2017/01/16/gradientdescent2/"><meta property="og:description" content=""><meta property="og:type" content="article"><meta property="article:published_time" content="2017-01-16"><meta property="article:author" content="Tiep Vu"><meta property="article:section" content="Optimization"><meta property="article:tag" content="GD"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Online-learning"><meta property="article:tag" content="Batch"><link rel="alternate" type="application/atom+xml" title="Tiep Vu's blog - Atom feed" href="/feed.xml"><!-- Google Analytics --><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-89509207-1', 'auto');
// ga('send', 'pageview');
ga('send', 'pageview', {
'page': '/2017/01/16/gradientdescent2/',
'title': 'B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)'
});
</script><!-- Google Tag Manager --><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-KTCD8BX');</script><!-- End Google Tag Manager --></head><body>
	<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) return;
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.9";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script><br><div class="container">
      	<div class="row">
	        <div class="col-md-2 hidden-xs hidden-sm">
	          	<a href="machinelearningcoban.html">
            <!-- <img width="80%" src="/images/logo.svg" /> -->
            <!-- <img width="100%" src="/images/logoTet.png" /> -->
            <!-- <img width="100%" src="/images/logo2.png" /> -->
            <!-- <img width="100%" style="padding-bottom: 3mm;" src="/images/logo_new.png" /> </a> -->
            <img width="100%" style="padding-bottom: 3mm;" src="images/latex-new_logo92.png"></a>
          <!-- <img width="100%" style="padding-bottom: 3mm;" src="/assets/latex/new_logo2_rau.png" /> </a> -->

            <br><a href="buymeacoffee.html">
            <img width="100%" style="padding-bottom: 3mm;" src="images/images-Buymeacoffee_blue.png"><br></a><a href="ebook.html">
            <img width="100%" style="padding-bottom: 3mm;" src="images/images-ebook_logo.png"><!-- <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#074B80', 
            'A40822MV');kofiwidget2.draw();</script>  --><!-- 
            <form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_top">
            <input type="hidden" name="cmd" value="_donations">
            <input type="hidden" name="business" value="vuhuutiep@gmail.com">
            <input type="hidden" name="lc" value="US">
            <input type="hidden" name="item_name" value="I find machinelearningcoban.com helpful. I'd like to buy Tiep Vu a coffee ^^. (Thank you so much for your support.)">
            <input type="hidden" name="no_note" value="0">
            <input type="hidden" name="currency_code" value="USD">
            <input type="hidden" name="bn" value="PP-DonationsBF:Buymeacoffee.png:NonHostedGuest">
            <input type="image" src="/images/Buymeacoffee_blue.png" border="0" style="padding-bottom: -9mm;" width = 100% name="submit" alt="PayPal - The safer, easier way to pay online!">
            </form> --><!-- <script type='text/javascript' src='https://ko-fi.com/widgets/widget_2.js'></script><script type='text/javascript'>kofiwidget2.init('Buy Me a Coffee', '#805007', 'A40822MV');kofiwidget2.draw();</script>  --></a>

          <!-- Google search -->
         <!--  <table border="0">
          <div id = "top-widget" style="width: 292px; margin-left: -13.5px; margin-top: -10px; margin-bottom: -15px;">
         <script>
           (function() {
             var cx = '012053542614118746585:ktgei4l2oek';
             var gcse = document.createElement('script');
             gcse.type = 'text/javascript';
             gcse.async = true;
             gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
             var s = document.getElementsByTagName('script')[0];
             s.parentNode.insertBefore(gcse, s);
           })();
         </script>
         <gcse:search></gcse:search>
          </div>
          </table> -->

          <!-- <nav>
          
            <div class="header">Popular</div>
            <ul>
              <li> (**): > 10k views</li>
              <li> (*) : > 5k views</li>
            </ul>
          </nav> -->
          

          
          <nav><div class="header">Latest by category</div>
            <ul><li><a style="text-align: left; color: #074B80;" href="duality.html">18. Duality</a></li>
                  
                    <li><a style="text-align: left; color: #074B80;" href="convexopt.html">17. Convex Optimization Problems</a></li>
                  
                    <li><a style="text-align: left; color: #074B80;" href="convexity.html">16. Convex sets v&agrave; convex functions</a></li>
                  
                    <li><a style="text-align: left; color: #074B80;" href="gradientdescent2.html">8. Gradient Descent (2/2)</a></li>
                  
                    <li><a style="text-align: left; color: #074B80;" href="gradientdescent.html">7. Gradient Descent (1/2)</a></li>
                  
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
            </ul></nav><nav><div class="header">Latest</div>
              
                <li><a style="text-align: left; color: #074B80" href="lifesofar2.html">Con &#273;&#432;&#7901;ng h&#7885;c PhD c&#7911;a t&ocirc;i</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="conv2d.html">37. T&iacute;ch ch&#7853;p hai chi&#7873;u</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="forum.html">Di&#7877;n &#273;&agrave;n</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="deeplearning.html">36. Keras</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="deeplearning.html">35. L&#432;&#7907;c s&#7917; Deep Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="phuonghoagiang.html">Con &#273;&#432;&#7901;ng h&#7885;c Khoa h&#7885;c d&#7919; li&#7879;u c&#7911;a m&#7897;t sinh vi&ecirc;n Kinh t&#7871;</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="id3.html">34. Decision Trees (1): ID3</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="evaluation.html">33. &#272;&aacute;nh gi&aacute; h&#7879; th&#7889;ng ph&acirc;n l&#7899;p</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="fundaml_vectors.html">FundaML 3: C&aacute;c m&#7843;ng ng&#7851;u nhi&ecirc;n</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="fundaml_matrices.html">FundaML 2: Ma tr&#7853;n</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="fundaml_vectors.html">FundaML 1: M&#7843;ng m&#7897;t chi&#7873;u</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="fundaml.html">FundaML.com</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="nbc.html">32. Naive Bayes Classifier</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="phdlife.html">Vi&#7871;t v&agrave; nh&#7853;n x&eacute;t c&aacute;c b&agrave;i b&aacute;o khoa h&#7885;c</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="mlemap.html">31. Maximum Likelihood v&agrave; Maximum A Posteriori</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="lifesofar.html">Con &#273;&#432;&#7901;ng h&#7885;c To&aacute;n c&#7911;a t&ocirc;i</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="prob.html">30. &Ocirc;n t&#7853;p X&aacute;c Su&#7845;t</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="tl.html">Q2. Transfer Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="lda.html">29. Linear Discriminant Analysis</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="qns1.html">Q1. Quick Notes 1</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="pca2.html">28. Principal Component Analysis (2/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="pca.html">27. Principal Component Analysis (1/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="svd.html">26. Singular Value Decomposition</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="matrixfactorization.html">25. Matrix Factorization Collaborative Filtering</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="collaborativefiltering.html">24. Neighborhood-Based Collaborative Filtering</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="contentbasedrecommendersys.html">23. Content-based Recommendation Systems</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="multiclasssmv.html">22. Multi-class SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="kernelsmv.html">21. Kernel SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="softmarginsmv.html">20. Soft Margin SVM</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="smv.html">19. Support Vector Machine</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="duality.html">18. Duality</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="convexopt.html">17. Convex Optimization Problems</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="convexity.html">16. Convex sets v&agrave; convex functions</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="overfitting.html">15. Overfitting</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="mlp.html">14. Multi-layer Perceptron v&agrave; Backpropagation</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="softmax.html">13. Softmax Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="binaryclassifiers.html">12. Binary Classifiers</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="featureengineering.html">11. Feature Engineering</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="howdoIcreatethisblog.html"></a></li>
              
                <li><a style="text-align: left; color: #074B80" href="logisticregression.html">10. Logistic Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="perceptron.html">9. Perceptron Learning Algorithm</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="gradientdescent2.html">8. Gradient Descent (2/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="gradientdescent.html">7. Gradient Descent (1/2)</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="knn.html">6. K-nearest neighbors</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="kmeans2.html">5. K-means Clustering - Applications</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="kmeans.html">4. K-means Clustering</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="linearregression.html">3. Linear Regression</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="categories.html">2. Ph&acirc;n nh&oacute;m c&aacute;c thu&#7853;t to&aacute;n Machine Learning</a></li>
              
                <li><a style="text-align: left; color: #074B80" href="introduce.html">1. Gi&#7899;i thi&#7879;u v&#7873; Machine Learning</a></li>
              
            
          </nav><!-- <img style = "transform: scaleX(1); width:100%; margin-left:00px;position: absolute;" src = "/images/mai.jpg"> --><!--   
            <nav>
              <div class="header">Previous by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2017/01/12/gradientdescent/">B&agrave;i 7: Gradient Descent (ph&#7847;n 1/2)</a></li>
              </ul>
            </nav>
           
           
            <nav>
              <div class="header">Next by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2017/01/21/perceptron/">B&agrave;i 9: Perceptron Learning Algorithm</a></li>
              </ul>
            </nav>
            --></div>
	        <div class="col-md-8 col-xs-12" style="z-index: 1">
	        	 <!-- <br> -->
 <nav class="navbar navbar-inverse" style="background-color: #074B80"><div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span> 
      </button>
      <a class="navbar-brand" href="machinelearningcoban.html"><span style="color: #fff">Machine Learning c&#417; b&#7843;n</span></a>
        <!-- <form class="navbar-form navbar-left" role="search">
            <div class="form-group" align="right">
                <input type="text" class="form-control" placeholder="Search">
            </div>
            <button type="submit" class="btn btn-default">
                <span></span>
            </button>
        </form> -->
        


    </div>
    <div class="collapse navbar-collapse navbar-right" id="myNavbar">
      <ul class="nav navbar-nav"><li><a href="about.html"><span style="color: #fff"> About</span></a></li>
        <li><a href="index.html"><span style="color: #fff">Index</span></a></li>
        <li><a href="tags.html"><span style="color: #fff">Tags</span></a></li>
        <li><a href="categories.html"><span style="color: #fff">Categories</span></a></li>
        <li><a href="archive.html"><span style="color: #fff">Archive</span></a></li>
        <li><a href="math.html"><span style="color: #fff">Math</span></a></li>
        <!-- <li><a href="https://docs.google.com/forms/d/e/1FAIpQLScq3GkxM1I2fDevR7gth-O9QqxM7grf4AFc0WT1hFORv4flaw/viewform"><span style = "color: #fff">Survey</span></a></li> -->
        <li><a href="copyrights.html"><span style="color: #fff">Copyrights</span></a></li>
        <!-- <li><a href="/faqs/"><span style = "color: #fff">FAQs</span></a></li> -->
        <li><a href="ebook.html"><span style="color: #fff">ebook</span></a></li>
        <li><a href="search.html"><span style="color: #fff">Search</span></a></li>
        <!-- <li><a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/latex/book.pdf"><span style = "color: #fff">Book</span></a></li> -->
        <!-- <li><a href="https://www.facebook.com/groups/257768141347267/"><span style = "color: #fff">Forum</span></a></li> -->
        <!-- <li><a href="/subscribe/">Subscribe</a></li> -->

        <li> 
      </li></ul></div>
  </div>
</nav><!-- <div class = "row"> --><!-- <div class = "col-xs-12 hidden-md hidden-lg"> --><!-- previous and next posts --><div class="PageNavigation">
         
            <a class="prev" style="color: #074B80;" href="gradientdescent.html">&laquo; B&agrave;i 7: Gradient Descent (ph&#7847;n 1/2)</a>
         <!-- <hr> -->
         
         
            <a class="next" style="float: right; color: #074B80;" href="perceptron.html">B&agrave;i 9: Perceptron Learning Algorithm &raquo;</a>
         <hr></div>
  <!-- </div> -->
<!-- </div> -->
<h1 itemprop="name" class="post-title">B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)</h1>


<ul class="tags"><a href="/tags#GD" class="tag">GD</a>
   
      <a href="/tags#Optimization" class="tag">Optimization</a>
   
      <a href="/tags#Online-learning" class="tag">Online-learning</a>
   
      <a href="/tags#Batch" class="tag">Batch</a>
   
</ul><span class="post-date" style="color: gray; font-style: italic;">Jan 16, 2017
            </span>
<!-- Main content -->
<br><br><div itemprop="articleBody">
   <div class="imgcap">
 <img src="images/09-contours_evaluation_optimizers.gif" align="center" width="400"><div class="thecap"> T&#7889;c &#273;&#7897; h&#7897;i t&#7909; c&#7911;a c&aacute;c thu&#7853;t to&aacute;n GD kh&aacute;c nhau. (Ngu&#7891;n <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent"></a> An overview of gradient descent optimization algorithms). </div>
</div>

<p>Trong <a href="gradientdescent.html">ph&#7847;n 1</a> c&#7911;a Gradient Descent (GD), t&ocirc;i &#273;&atilde; gi&#7899;i thi&#7879;u v&#7899;i b&#7841;n &#273;&#7885;c v&#7873; thu&#7853;t to&aacute;n Gradient Descent. T&ocirc;i xin nh&#7855;c l&#7841;i r&#7857;ng nghi&#7879;m cu&#7889;i c&ugrave;ng c&#7911;a Gradient Descent ph&#7909; thu&#7897;c r&#7845;t nhi&#7873;u v&agrave;o &#273;i&#7875;m kh&#7903;i t&#7841;o v&agrave; learning rate. Trong b&agrave;i n&agrave;y, t&ocirc;i xin &#273;&#7873; c&#7853;p m&#7897;t v&agrave;i ph&#432;&#417;ng ph&aacute;p th&#432;&#7901;ng &#273;&#432;&#7907;c d&ugrave;ng &#273;&#7875; kh&#7855;c ph&#7909;c nh&#7919;ng h&#7841;n ch&#7871; c&#7911;a GD. &#272;&#7891;ng th&#7901;i, c&aacute;c thu&#7853;t to&aacute;n bi&#7871;n th&#7875; c&#7911;a GD th&#432;&#7901;ng &#273;&#432;&#7907;c &aacute;p d&#7909;ng trong c&aacute;c m&ocirc; h&igrave;nh Deep Learning c&#361;ng s&#7869; &#273;&#432;&#7907;c t&#7893;ng h&#7907;p.</p>

<p><strong>Trong trang n&agrave;y:</strong>
<!-- MarkdownTOC --></p>

<ul><li><a href="#-cac-thuat-toan-toi-uu-gradient-descent">1. C&aacute;c thu&#7853;t to&aacute;n t&#7889;i &#432;u Gradient Descent</a>
    <ul><li><a href="#-momentum">1.1 Momentum</a>
        <ul><li><a href="#nhac-lai-thuat-toan-gradient-descent">Nh&#7855;c l&#7841;i thu&#7853;t to&aacute;n Gradient Descent</a></li>
          <li><a href="#gradient-duoi-goc-nhin-vat-ly">Gradient d&#432;&#7899;i g&oacute;c nh&igrave;n v&#7853;t l&yacute;</a></li>
          <li><a href="#gradient-descent-voi-momentum">Gradient Descent v&#7899;i Momentum</a></li>
          <li><a href="#mot-vi-du-nho">M&#7897;t v&iacute; d&#7909; nh&#7887;</a></li>
        </ul></li>
      <li><a href="#-nesterov-accelerated-gradient-nag">1.2. Nesterov accelerated gradient (NAG)</a>
        <ul><li><a href="#y-tuong-chinh">&Yacute; t&#432;&#7903;ng ch&iacute;nh</a></li>
          <li><a href="#cong-thuc-cap-nhat">C&ocirc;ng th&#7913;c c&#7853;p nh&#7853;t</a></li>
          <li><a href="#vi-du-minh-hoa">V&iacute; d&#7909; minh h&#7885;a</a></li>
        </ul></li>
      <li><a href="#-cac-thuat-toan-khac">1.3. C&aacute;c thu&#7853;t to&aacute;n kh&aacute;c</a></li>
    </ul></li>
  <li><a href="#-bien-the-cua-gradient-descent">2. Bi&#7871;n th&#7875; c&#7911;a Gradient Descent</a>
    <ul><li><a href="#-batch-gradient-descent">2.1. Batch Gradient Descent</a></li>
      <li><a href="#-stochastic-gradient-descent">2.2. Stochastic Gradient Descent.</a>
        <ul><li><a href="#vi-du-voi-bai-toan-linear-regression">V&iacute; d&#7909; v&#7899;i b&agrave;i to&aacute;n Linear Regression</a></li>
        </ul></li>
      <li><a href="#-mini-batch-gradient-descent">2.3. Mini-batch Gradient Descent</a></li>
    </ul></li>
  <li><a href="#-stopping-criteria-dieu-kien-dung">3. Stopping Criteria (&#273;i&#7873;u ki&#7879;n d&#7915;ng)</a></li>
  <li><a href="#-mot-phuong-phap-toi-uu-don-gian-khac-newtons-method">4. M&#7897;t ph&#432;&#417;ng ph&aacute;p t&#7889;i &#432;u &#273;&#417;n gi&#7843;n kh&aacute;c: Newton&rsquo;s method</a>
    <ul><li><a href="#newtons-method-cho-giai-phuong-trinh-%5C%5Cfx--%5C%5C">Newton&rsquo;s method cho gi&#7843;i ph&#432;&#417;ng tr&igrave;nh \(f(x) = 0\)</a></li>
      <li><a href="#newtons-method-trong-bai-toan-tim-local-minimun">Newton&rsquo;s method trong b&agrave;i to&aacute;n t&igrave;m local minimun</a></li>
      <li><a href="#han-che-cua-newtons-method">H&#7841;n ch&#7871; c&#7911;a Newton&rsquo;s method</a></li>
    </ul></li>
  <li><a href="#-ket-luan">5. K&#7871;t lu&#7853;n</a></li>
  <li><a href="#-tai-lieu-tham-khao">6. T&agrave;i li&#7879;u tham kh&#7843;o</a></li>
</ul><!-- /MarkdownTOC --><p><a name="-cac-thuat-toan-toi-uu-gradient-descent" href="gradientdescent2.html"></a></p>

<h2 id="1-c&aacute;c-thu&#7853;t-to&aacute;n-t&#7889;i-&#432;u-gradient-descent">1. C&aacute;c thu&#7853;t to&aacute;n t&#7889;i &#432;u Gradient Descent</h2>

<p><a name="-momentum" href="gradientdescent2.html"></a></p>

<h3 id="11-momentum">1.1 Momentum</h3>
<p><a name="nhac-lai-thuat-toan-gradient-descent" href="gradientdescent2.html"></a></p>

<h4 id="nh&#7855;c-l&#7841;i-thu&#7853;t-to&aacute;n-gradient-descent">Nh&#7855;c l&#7841;i thu&#7853;t to&aacute;n Gradient Descent</h4>
<p>D&agrave;nh cho c&aacute;c b&#7841;n ch&#432;a &#273;&#7885;c <a href="gradientdescent.html">ph&#7847;n 1</a> c&#7911;a Gradient Descent. &#272;&#7875; gi&#7843;i b&agrave;i to&aacute;n t&igrave;m &#273;i&#7875;m <em>global optimal</em> c&#7911;a h&agrave;m m&#7845;t m&aacute;t \(J(\theta)\) (H&agrave;m m&#7845;t m&aacute;t c&#361;ng th&#432;&#7901;ng &#273;&#432;&#7907;c k&yacute; hi&#7879;u l&agrave; \(J()\) v&#7899;i \(\theta\) l&agrave; t&#7853;p h&#7907;p c&aacute;c tham s&#7889; c&#7911;a m&ocirc; h&igrave;nh), t&ocirc;i xin nh&#7855;c l&#7841;i thu&#7853;t to&aacute;n GD:</p>

<hr><p><strong>Thu&#7853;t to&aacute;n Gradient Descent:</strong></p>

<ol><li>D&#7921; &#273;o&aacute;n m&#7897;t &#273;i&#7875;m kh&#7903;i t&#7841;o \(\theta = \theta_0\).</li>
  <li>C&#7853;p nh&#7853;t \(\theta\) &#273;&#7871;n khi &#273;&#7841;t &#273;&#432;&#7907;c k&#7871;t qu&#7843; ch&#7845;p nh&#7853;n &#273;&#432;&#7907;c: 
\[
\theta = \theta - \eta \nabla_{\theta}J(\theta)
\]</li>
</ol><p><a name="voi-%5C%5C%5Cnabla%5Cthetaj%5Ctheta%5C%5C-la-dao-ham-cua-ham-mat-mat-tai-%5C%5C%5Ctheta%5C%5C" href="gradientdescent2.html"></a></p>

<p>v&#7899;i \(\nabla_{\theta}J(\theta)\) l&agrave; &#273;&#7841;o h&agrave;m c&#7911;a h&agrave;m m&#7845;t m&aacute;t t&#7841;i \(\theta\).</p>

<hr><p><a name="gradient-duoi-goc-nhin-vat-ly" href="gradientdescent2.html"></a></p>

<h4 id="gradient-d&#432;&#7899;i-g&oacute;c-nh&igrave;n-v&#7853;t-l&yacute;">Gradient d&#432;&#7899;i g&oacute;c nh&igrave;n v&#7853;t l&yacute;</h4>

<p>Thu&#7853;t to&aacute;n GD th&#432;&#7901;ng &#273;&#432;&#7907;c v&iacute; v&#7899;i t&aacute;c d&#7909;ng c&#7911;a tr&#7885;ng l&#7921;c l&ecirc;n m&#7897;t h&ograve;n bi &#273;&#7863;t tr&ecirc;n m&#7897;t m&#7863;t c&oacute; d&#7841;ng nh&#432; h&igrave;nh m&#7897;t thung l&#361;ng gi&#7889;ng nh&#432; h&igrave;nh 1a) d&#432;&#7899;i &#273;&acirc;y. B&#7845;t k&#7875; ta &#273;&#7863;t h&ograve;n bi &#7903; A hay B th&igrave; cu&#7889;i c&ugrave;ng h&ograve;n bi c&#361;ng s&#7869; l&#259;n xu&#7889;ng v&agrave; k&#7871;t th&uacute;c &#7903; v&#7883; tr&iacute; C.</p>

<div class="imgcap">
 <img src="images/GD-momentum.png" align="center" width="800"><div class="thecap"> H&igrave;nh 1: So s&aacute;nh Gradient Descent v&#7899;i c&aacute;c hi&#7879;n t&#432;&#7907;ng v&#7853;t l&yacute; </div>
</div>

<p>Tuy nhi&ecirc;n, n&#7871;u nh&#432; b&#7873; m&#7863;t c&oacute; hai &#273;&aacute;y thung l&#361;ng nh&#432; H&igrave;nh 1b) th&igrave; t&ugrave;y v&agrave;o vi&#7879;c &#273;&#7863;t bi &#7903; A hay B, v&#7883; tr&iacute; cu&#7889;i c&ugrave;ng c&#7911;a bi s&#7869; &#7903; C ho&#7863;c D. &#272;i&#7875;m D l&agrave; m&#7897;t &#273;i&#7875;m local minimum ch&uacute;ng ta kh&ocirc;ng mong mu&#7889;n.</p>

<p>N&#7871;u suy ngh&#297; m&#7897;t c&aacute;ch v&#7853;t l&yacute; h&#417;n, v&#7851;n trong H&igrave;nh 1b), n&#7871;u v&#7853;n t&#7889;c ban &#273;&#7847;u c&#7911;a bi khi &#7903; &#273;i&#7875;m B &#273;&#7911; l&#7899;n, khi bi l&#259;n &#273;&#7871;n &#273;i&#7875;m D, theo <em>&#273;&agrave;</em>, bi c&oacute; th&#7875; ti&#7871;p t&#7909;c di chuy&#7875;n l&ecirc;n d&#7889;c ph&iacute;a b&ecirc;n tr&aacute;i c&#7911;a D. V&agrave; n&#7871;u gi&#7843; s&#7917; v&#7853;n t&#7889;c ban &#273;&#7847;u l&#7899;n h&#417;n n&#7919;a, bi c&oacute; th&#7875; v&#432;&#7907;t d&#7889;c t&#7899;i &#273;i&#7875;m E r&#7891;i l&#259;n xu&#7889;ng C nh&#432; trong H&igrave;nh 1c). &#272;&acirc;y ch&iacute;nh l&agrave; &#273;i&#7873;u ch&uacute;ng ta mong mu&#7889;n. B&#7841;n &#273;&#7885;c c&oacute; th&#7875; &#273;&#7863;t c&acirc;u h&#7887;i r&#7857;ng li&#7879;u bi l&#259;n t&#7915; A t&#7899;i C c&oacute; theo <em>&#273;&agrave;</em> l&#259;n t&#7899;i E r&#7891;i t&#7899;i D kh&ocirc;ng. Xin tr&#7843; l&#7901;i r&#7857;ng &#273;i&#7873;u n&agrave;y kh&oacute; x&#7843;y ra h&#417;n v&igrave; n&#7871;u so v&#7899;i d&#7889;c DE th&igrave; d&#7889;c CE cao h&#417;n nhi&#7873;u.</p>

<p>D&#7921;a tr&ecirc;n hi&#7879;n t&#432;&#7907;ng n&agrave;y, m&#7897;t thu&#7853;t to&aacute;n &#273;&#432;&#7907;c ra &#273;&#7901;i nh&#7857;m kh&#7855;c ph&#7909;c vi&#7879;c nghi&#7879;m c&#7911;a GD r&#417;i v&agrave;o m&#7897;t &#273;i&#7875;m local minimum kh&ocirc;ng mong mu&#7889;n. Thu&#7853;t to&aacute;n &#273;&oacute; c&oacute; t&ecirc;n l&agrave; Momentum (t&#7913;c <em>theo &#273;&agrave;</em> trong ti&#7871;ng Vi&#7879;t).</p>

<p><a name="gradient-descent-voi-momentum" href="gradientdescent2.html"></a></p>

<h4 id="gradient-descent-v&#7899;i-momentum">Gradient Descent v&#7899;i Momentum</h4>
<p>&#272;&#7875; bi&#7875;u di&#7877;n <em>momentum</em> b&#7857;ng to&aacute;n h&#7885;c th&igrave; ch&uacute;ng ta ph&#7843;i l&agrave;m th&#7871; n&agrave;o?</p>

<p>Trong GD, ch&uacute;ng ta c&#7847;n t&iacute;nh l&#432;&#7907;ng thay &#273;&#7893;i &#7903; th&#7901;i &#273;i&#7875;m \(t\) &#273;&#7875; c&#7853;p nh&#7853;t v&#7883; tr&iacute; m&#7899;i cho nghi&#7879;m (t&#7913;c <em>h&ograve;n bi</em>). N&#7871;u ch&uacute;ng ta coi &#273;&#7841;i l&#432;&#7907;ng n&agrave;y nh&#432; v&#7853;n t&#7889;c \(v_t\) trong v&#7853;t l&yacute;, v&#7883; tr&iacute; m&#7899;i c&#7911;a <em>h&ograve;n bi</em> s&#7869; l&agrave; \(\theta_{t+1} = \theta_{t} - v_t\). D&#7845;u tr&#7915; th&#7875; hi&#7879;n vi&#7879;c ph&#7843;i di chuy&#7875;n ng&#432;&#7907;c v&#7899;i &#273;&#7841;o h&agrave;m. C&ocirc;ng vi&#7879;c c&#7911;a ch&uacute;ng ta b&acirc;y gi&#7901; l&agrave; t&iacute;nh &#273;&#7841;i l&#432;&#7907;ng \(v_t\) sao cho n&oacute; v&#7915;a mang th&ocirc;ng tin c&#7911;a <em>&#273;&#7897; d&#7889;c</em> (t&#7913;c &#273;&#7841;o h&agrave;m), v&#7915;a mang th&ocirc;ng tin c&#7911;a <em>&#273;&agrave;</em>, t&#7913;c v&#7853;n t&#7889;c tr&#432;&#7899;c &#273;&oacute; \(v_{t-1}\) (ch&uacute;ng ta coi nh&#432; v&#7853;n t&#7889;c ban &#273;&#7847;u \(v_0=0\)). M&#7897;t c&aacute;ch &#273;&#417;n gi&#7843;n nh&#7845;t, ta c&oacute; th&#7875; c&#7897;ng (c&oacute; tr&#7885;ng s&#7889;) hai &#273;&#7841;i l&#432;&#7907;ng n&agrave;y l&#7841;i:
\[
v_{t}= \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta)
\]</p>

<p>Trong &#273;&oacute; \(\gamma\) th&#432;&#7901;ng &#273;&#432;&#7907;c ch&#7885;n l&agrave; m&#7897;t gi&aacute; tr&#7883; kho&#7843;ng 0.9, \(v_t\) l&agrave; v&#7853;n t&#7889;c t&#7841;i th&#7901;i &#273;i&#7875;m tr&#432;&#7899;c &#273;&oacute;, \( \nabla_{\theta}J(\theta)\) ch&iacute;nh l&agrave; &#273;&#7897; d&#7889;c c&#7911;a &#273;i&#7875;m tr&#432;&#7899;c &#273;&oacute;. 
Sau &#273;&oacute; v&#7883; tr&iacute; m&#7899;i c&#7911;a <em>h&ograve;n bi</em> &#273;&#432;&#7907;c x&aacute;c &#273;&#7883;nh nh&#432; sau:
\[
\theta = \theta - v_t
\]</p>

<p>Thu&#7853;t to&aacute;n &#273;&#417;n gi&#7843;n n&agrave;y t&#7887; ra r&#7845;t hi&#7879;u qu&#7843; trong c&aacute;c b&agrave;i to&aacute;n th&#7921;c t&#7871; (trong kh&ocirc;ng gian nhi&#7873;u chi&#7873;u, c&aacute;ch t&iacute;nh to&aacute;n c&#361;ng ho&agrave;n t&ograve;an t&#432;&#417;ng t&#7921;). D&#432;&#7899;i &#273;&acirc;y l&agrave; m&#7897;t v&iacute; d&#7909; trong kh&ocirc;ng gian m&#7897;t chi&#7873;u.</p>

<p><a name="mot-vi-du-nho" href="gradientdescent2.html"></a></p>

<h4 id="m&#7897;t-v&iacute;-d&#7909;-nh&#7887;">M&#7897;t v&iacute; d&#7909; nh&#7887;</h4>
<p>Ch&uacute;ng ta xem x&eacute;t m&#7897;t h&agrave;m &#273;&#417;n gi&#7843;n c&oacute; hai &#273;i&#7875;m local minimum, trong &#273;&oacute; 1 &#273;i&#7875;m l&agrave; global minimum:
\[
f(x) = x^2 + 10\sin(x)
\]
C&oacute; &#273;&#7841;o h&agrave;m l&agrave;: \(f&rsquo;(x) = 2x + 10\cos(x)\). H&igrave;nh 2 d&#432;&#7899;i &#273;&acirc;y th&#7875; hi&#7879;n s&#7921; kh&aacute;c nhau gi&#7919;a thu&#7853;t to&aacute;n GD v&agrave; thu&#7853;t to&aacute;n GD v&#7899;i Momentum:</p>

<div>
<table width="100%" style="border: 0px solid white"><tr><td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="images/GD-nomomentum1d.gif"></td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="images/GD-momentum1d.gif"></td>
    </tr></table><div class="thecap"> H&igrave;nh 2: Minh h&#7885;a thu&#7853;t to&aacute;n GD v&#7899;i Momentum. </div>
</div>

<p>H&igrave;nh b&ecirc;n tr&aacute;i l&agrave; &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m khi kh&ocirc;ng s&#7917; d&#7909;ng Momentum, thu&#7853;t to&aacute;n h&#7897;i t&#7909; sau ch&#7881; 5 v&ograve;ng l&#7863;p nh&#432;ng nghi&#7879;m t&igrave;m &#273;&#432;&#7907;c l&agrave; nghi&#7879;m local minimun.</p>

<p>H&igrave;nh b&ecirc;n ph&#7843;i l&agrave; &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m khi c&oacute; s&#7917; d&#7909;ng Momentum, <em>h&ograve;n bi</em> &#273;&atilde; c&oacute; th&#7875; v&#432;&#7907;t d&#7889;c t&#7899;i khu v&#7921;c g&#7847;n &#273;i&#7875;m global minimun, sau &#273;&oacute; dao &#273;&#7897;ng xung quanh &#273;i&#7875;m n&agrave;y, gi&#7843;m t&#7889;c r&#7891;i cu&#7889;i c&ugrave;ng t&#7899;i &#273;&iacute;ch. M&#7863;c d&ugrave; m&#7845;t nhi&#7873;u v&ograve;ng l&#7863;p h&#417;n, GD v&#7899;i Momentum cho ch&uacute;ng ta nghi&#7879;m ch&iacute;nh x&aacute;c h&#417;n. Quan s&aacute;t &#273;&#432;&#7901;ng &#273;i c&#7911;a <em>h&ograve;n bi</em> trong tr&#432;&#7901;ng h&#7907;p n&agrave;y, ch&uacute;ng ta th&#7845;y r&#7857;ng &#273;i&#7873;u n&agrave;y gi&#7889;ng v&#7899;i v&#7853;t l&yacute; h&#417;n!</p>

<p>N&#7871;u bi&#7871;t tr&#432;&#7899;c &#273;i&#7875;m <em>&#273;&#7863;t bi</em> ban &#273;&#7847;u <code class="language-plaintext highlighter-rouge">theta</code>, &#273;&#7841;o h&agrave;m c&#7911;a h&agrave;m m&#7845;t m&aacute;t t&#7841;i m&#7897;t &#273;i&#7875;m b&#7845;t k&#7923; <code class="language-plaintext highlighter-rouge">grad(theta)</code>, l&#432;&#7907;ng th&ocirc;ng tin l&#432;u tr&#7919; t&#7915; v&#7853;n t&#7889;c tr&#432;&#7899;c &#273;&oacute; <code class="language-plaintext highlighter-rouge">gamma</code> v&agrave; learning rate <code class="language-plaintext highlighter-rouge">eta</code>, ch&uacute;ng ta c&oacute; th&#7875; vi&#7871;t h&agrave;m s&#7889; <code class="language-plaintext highlighter-rouge">GD_momentum</code> trong Python nh&#432; sau:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check convergence
</span><span class="k">def</span> <span class="nf">has_converged</span><span class="p">(</span><span class="n">theta_new</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">theta_new</span><span class="p">))</span><span class="o">/</span>
                            <span class="nb">len</span><span class="p">(</span><span class="n">theta_new</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-3</span>

<span class="k">def</span> <span class="nf">GD_momentum</span><span class="p">(</span><span class="n">theta_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="c1"># Suppose we want to store history of theta
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="p">[</span><span class="n">theta_init</span><span class="p">]</span>
    <span class="n">v_old</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">theta_init</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">v_new</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">*</span><span class="n">v_old</span> <span class="o">+</span> <span class="n">eta</span><span class="o">*</span><span class="n">grad</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">theta_new</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">v_new</span>
        <span class="k">if</span> <span class="n">has_converged</span><span class="p">(</span><span class="n">theta_new</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
            <span class="k">break</span> 
        <span class="n">theta</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">theta_new</span><span class="p">)</span>
        <span class="n">v_old</span> <span class="o">=</span> <span class="n">v_new</span>
    <span class="k">return</span> <span class="n">theta</span> 
    <span class="c1"># this variable includes all points in the path
</span>    <span class="c1"># if you just want the final answer, 
</span>    <span class="c1"># use `return theta[-1]`
</span></code></pre></div></div>

<p><a name="-nesterov-accelerated-gradient-nag" href="gradientdescent2.html"></a></p>

<h3 id="12-nesterov-accelerated-gradient-nag">1.2. Nesterov accelerated gradient (NAG)</h3>

<p>Momentum gi&uacute;p <em>h&ograve;n bi</em> v&#432;&#7907;t qua &#273;&#432;&#7907;c <em>d&#7889;c locaminimum</em>, tuy nhi&ecirc;n, c&oacute; m&#7897;t h&#7841;n ch&#7871; ch&uacute;ng ta c&oacute; th&#7875; th&#7845;y trong v&iacute; d&#7909; tr&ecirc;n: Khi t&#7899;i g&#7847;n <em>&#273;&iacute;ch</em>, momemtum v&#7851;n m&#7845;t kh&aacute; nhi&#7873;u th&#7901;i gian tr&#432;&#7899;c khi d&#7915;ng l&#7841;i. L&yacute; do l&#7841;i c&#361;ng ch&iacute;nh l&agrave; v&igrave; c&oacute; <em>&#273;&agrave;</em>. C&oacute; m&#7897;t ph&#432;&#417;ng ph&aacute;p kh&aacute;c ti&#7871;p t&#7909;c gi&uacute;p kh&#7855;c ph&#7909;c &#273;i&#7873;u n&agrave;y, ph&#432;&#417;ng ph&aacute;p &#273;&oacute; mang t&ecirc;n Nesterov accelerated gradient (NAG), gi&uacute;p cho thu&#7853;t to&aacute;n h&#7897;i t&#7909; nhanh h&#417;n.</p>

<p><a name="y-tuong-chinh" href="gradientdescent2.html"></a></p>

<h4 id="&yacute;-t&#432;&#7903;ng-ch&iacute;nh">&Yacute; t&#432;&#7903;ng ch&iacute;nh</h4>

<p>&Yacute; t&#432;&#7903;ng c&#417; b&#7843;n l&agrave; <em>d&#7921; &#273;o&aacute;n h&#432;&#7899;ng &#273;i trong t&#432;&#417;ng lai</em>, t&#7913;c nh&igrave;n tr&#432;&#7899;c m&#7897;t b&#432;&#7899;c! C&#7909; th&#7875;, n&#7871;u s&#7917; d&#7909;ng s&#7889; h&#7841;ng <em>momentum</em> \(\gamma v_{t-1}\) &#273;&#7875; c&#7853;p nh&#7853;t th&igrave; ta c&oacute; th&#7875; <em>x&#7845;p x&#7881;</em> &#273;&#432;&#7907;c v&#7883; tr&iacute; ti&#7871;p theo c&#7911;a h&ograve;n bi l&agrave; \(\theta - \gamma v_{t-1}\) (ch&uacute;ng ta kh&ocirc;ng &#273;&iacute;nh k&egrave;m ph&#7847;n gradient &#7903; &#273;&acirc;y v&igrave; s&#7869; s&#7917; d&#7909;ng n&oacute; trong b&#432;&#7899;c cu&#7889;i c&ugrave;ng). V&#7853;y, thay v&igrave; s&#7917; d&#7909;ng gradient c&#7911;a &#273;i&#7875;m hi&#7879;n t&#7841;i, NAG <em>&#273;i tr&#432;&#7899;c m&#7897;t b&#432;&#7899;c</em>, s&#7917; d&#7909;ng gradient c&#7911;a &#273;i&#7875;m ti&#7871;p theo. Theo d&otilde;i h&igrave;nh d&#432;&#7899;i &#273;&acirc;y:</p>

<div class="imgcap">
 <img src="images/GD-nesterov.jpeg" align="center" width="800"><div class="thecap"> &Yacute; t&#432;&#7903;ng c&#7911;a Nesterov accelerated gradient. (Ngu&#7891;n: <a href="https://cs231n.github.io/neural-networks-3/">CS231n Stanford: Convolutional Neural Networks for Visual Recognition</a>) </div>
</div>

<ul><li>
    <p>V&#7899;i momentum th&ocirc;ng th&#432;&#7901;ng: <em>l&#432;&#7907;ng thay &#273;&#7893;i</em> l&agrave; t&#7893;ng c&#7911;a hai vector: momentum vector v&agrave; gradient &#7903; th&#7901;i &#273;i&#7875;m hi&#7879;n t&#7841;i.</p>
  </li>
  <li>
    <p>V&#7899;i Nesterove momentum: <em>l&#432;&#7907;ng thay &#273;&#7893;i</em> l&agrave; t&#7893;ng c&#7911;a hai vector: momentum vector v&agrave; gradient &#7903; th&#7901;i &#273;i&#7875;m &#273;&#432;&#7907;c x&#7845;p x&#7881; l&agrave; &#273;i&#7875;m ti&#7871;p theo.</p>
  </li>
</ul><p><a name="cong-thuc-cap-nhat" href="gradientdescent2.html"></a></p>

<h4 id="c&ocirc;ng-th&#7913;c-c&#7853;p-nh&#7853;t">C&ocirc;ng th&#7913;c c&#7853;p nh&#7853;t</h4>

<p>C&ocirc;ng th&#7913;c c&#7853;p nh&#7853;t c&#7911;a NAG &#273;&#432;&#7907;c cho nh&#432; sau:</p>

<p>\[
\begin{eqnarray}
v_{t} &amp;=&amp; \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta - \gamma v_{t-1}) \<br>
\theta &amp;=&amp; \theta -  v_{t}
\end{eqnarray}
\]</p>

<p>&#272;&#7875; &yacute; m&#7897;t ch&uacute;t c&aacute;c b&#7841;n s&#7869; th&#7845;y &#273;i&#7875;m &#273;&#432;&#7907;c t&iacute;nh &#273;&#7841;o h&agrave;m &#273;&atilde; thay &#273;&#7893;i.</p>

<p><a name="vi-du-minh-hoa" href="gradientdescent2.html"></a></p>

<h4 id="v&iacute;-d&#7909;-minh-h&#7885;a">V&iacute; d&#7909; minh h&#7885;a</h4>

<p>D&#432;&#7899;i &#273;&acirc;y l&agrave; v&iacute; d&#7909; so s&aacute;nh Momentum v&agrave; NAG cho b&agrave;i to&aacute;n Linear Regression:</p>

<div>
<table width="100%" style="border: 0px solid white"><tr><td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="images/GD-LR_momentum_contours.gif"></td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="images/GD-LR_NAG_contours.gif"></td>
    </tr></table><div class="thecap"> Minh h&#7885;a thu&#7853;t to&aacute;n GD v&#7899;i Momentum v&agrave; NAG. </div>
</div>

<p>H&igrave;nh b&ecirc;n tr&aacute;i l&agrave; &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m v&#7899;i ph&#432;&#417;ng ph&aacute;p Momentum. nghi&#7879;m &#273;i kh&aacute; l&agrave; <em>zigzag</em> v&agrave; m&#7845;t nhi&#7873;u v&ograve;ng l&#7863;p h&#417;n. H&igrave;nh b&ecirc;n ph&#7843;i l&agrave; &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m v&#7899;i ph&#432;&#417;ng ph&aacute;p NAG, nghi&#7879;m h&#7897;i t&#7909; nhanh h&#417;n, v&agrave; &#273;&#432;&#7901;ng &#273;i &iacute;t <em>zigzag</em> h&#417;n.</p>

<p>(Source code cho <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/LR%20Momentum.ipynb">h&igrave;nh b&ecirc;n tr&aacute;i</a> v&agrave; <a href="https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/LR%20NAG.ipynb"> h&igrave;nh b&ecirc;n ph&#7843;i</a>).</p>

<p><a name="-cac-thuat-toan-khac" href="gradientdescent2.html"></a></p>

<h3 id="13-c&aacute;c-thu&#7853;t-to&aacute;n-kh&aacute;c">1.3. C&aacute;c thu&#7853;t to&aacute;n kh&aacute;c</h3>
<p>Ngo&agrave;i hai thu&#7853;t to&aacute;n tr&ecirc;n, c&oacute; r&#7845;t nhi&#7873;u thu&#7853;t to&aacute;n n&acirc;ng cao kh&aacute;c &#273;&#432;&#7907;c s&#7917; d&#7909;ng trong c&aacute;c b&agrave;i to&aacute;n th&#7921;c t&#7871;, &#273;&#7863;c bi&#7879;t l&agrave; c&aacute;c b&agrave;i to&aacute;n Deep Learning. C&oacute; th&#7875; n&ecirc;u m&#7897;t v&agrave;i t&#7915; kh&oacute;a nh&#432; Adagrad, Adam, RMSprop,&hellip; T&ocirc;i s&#7869; kh&ocirc;ng &#273;&#7873; c&#7853;p &#273;&#7871;n c&aacute;c thu&#7853;t to&aacute;n &#273;&oacute; trong b&agrave;i n&agrave;y m&agrave; s&#7869; d&agrave;nh th&#7901;i gian n&oacute;i t&#7899;i n&#7871;u c&oacute; d&#7883;p trong t&#432;&#417;ng lai, khi blog &#273;&atilde; &#273;&#7911; l&#7899;n v&agrave; &#273;&atilde; trang b&#7883; cho c&aacute;c b&#7841;n m&#7897;t l&#432;&#7907;ng ki&#7871;n th&#7913;c nh&#7845;t &#273;&#7883;nh.</p>

<p>Tuy nhi&ecirc;n, b&#7841;n &#273;&#7885;c n&agrave;o mu&#7889;n &#273;&#7885;c th&ecirc;m c&oacute; th&#7875; t&igrave;m &#273;&#432;&#7907;c r&#7845;t nhi&#7873;u th&ocirc;ng tin h&#7919;u &iacute;ch trong b&agrave;i n&agrave;y:
<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">An overview of gradient descent optimization algorithms </a>.</p>

<p><a name="-bien-the-cua-gradient-descent" href="gradientdescent2.html"></a></p>

<h2 id="2-bi&#7871;n-th&#7875;-c&#7911;a-gradient-descent">2. Bi&#7871;n th&#7875; c&#7911;a Gradient Descent</h2>
<p>T&ocirc;i xin m&#7897;t l&#7847;n n&#7919;a d&ugrave;ng b&agrave;i to&aacute;n <a href="linearregression.html">Linear Regression</a> l&agrave;m v&iacute; d&#7909;. H&agrave;m m&#7845;t m&aacute;t v&agrave; &#273;&#7841;o h&agrave;m c&#7911;a n&oacute; cho b&agrave;i to&aacute;n n&agrave;y l&#7847;n l&#432;&#7907;t l&agrave; (&#273;&#7875; cho thu&#7853;n ti&#7879;n, trong b&agrave;i n&agrave;y t&ocirc;i s&#7869; d&ugrave;ng k&yacute; hi&#7879;u \(\mathbf{X}\) thay cho d&#7919; li&#7879;u m&#7903; r&#7897;ng \(\bar{\mathbf{X}}\)):</p>

<p>\[
J(\mathbf{w}) = \frac{1}{2N}||\mathbf{X}\mathbf{w} - \mathbf{y}||_2^2
\]
\[
~~~~ = \frac{1}{2N} \sum_{i=1}^N(\mathbf{x}_i \mathbf{w} - y_i)^2
\]
v&agrave;:
\[
\nabla_{\mathbf{w}} J(\mathbf{w}) = \frac{1}{N}\sum_{i=1}^N \mathbf{x}_i^T(\mathbf{x}_i\mathbf{w} - y_i)
\]</p>

<p><a name="-batch-gradient-descent" href="gradientdescent2.html"></a></p>

<h3 id="21-batch-gradient-descent">2.1. Batch Gradient Descent</h3>
<p>Thu&#7853;t to&aacute;n Gradient Descent ch&uacute;ng ta n&oacute;i t&#7915; &#273;&#7847;u ph&#7847;n 1 &#273;&#7871;n gi&#7901; c&ograve;n &#273;&#432;&#7907;c g&#7885;i l&agrave; Batch Gradient Descent. Batch &#7903; &#273;&acirc;y &#273;&#432;&#7907;c hi&#7875;u l&agrave; <em>t&#7845;t c&#7843;</em>, t&#7913;c khi c&#7853;p nh&#7853;t \(\theta = \mathbf{w}\), ch&uacute;ng ta s&#7917; d&#7909;ng <strong>t&#7845;t c&#7843;</strong> c&aacute;c &#273;i&#7875;m d&#7919; li&#7879;u \(\mathbf{x}_i\).</p>

<p>C&aacute;ch l&agrave;m n&agrave;y c&oacute; m&#7897;t v&agrave;i h&#7841;n ch&#7871; &#273;&#7889;i v&#7899;i c&#417; s&#7903; d&#7919; li&#7879;u c&oacute; v&ocirc; c&ugrave;ng nhi&#7873;u &#273;i&#7875;m (h&#417;n 1 t&#7881; ng&#432;&#7901;i d&ugrave;ng c&#7911;a facebook ch&#7859;ng h&#7841;n). Vi&#7879;c ph&#7843;i t&iacute;nh to&aacute;n l&#7841;i &#273;&#7841;o h&agrave;m v&#7899;i t&#7845;t c&#7843; c&aacute;c &#273;i&#7875;m n&agrave;y sau m&#7895;i v&ograve;ng l&#7863;p tr&#7903; n&ecirc;n c&#7891;ng k&#7873;nh v&agrave; kh&ocirc;ng hi&#7879;u qu&#7843;. Th&ecirc;m n&#7919;a, thu&#7853;t to&aacute;n n&agrave;y &#273;&#432;&#7907;c coi l&agrave; kh&ocirc;ng hi&#7879;u qu&#7843; v&#7899;i <em>online learning</em>.</p>

<p><a name="online-learning" href="gradientdescent2.html"></a></p>

<p><strong>Online learning</strong> l&agrave; khi c&#417; s&#7903; d&#7919; li&#7879;u &#273;&#432;&#7907;c c&#7853;p nh&#7853;t li&ecirc;n t&#7909;c (th&ecirc;m ng&#432;&#7901;i d&ugrave;ng &#273;&#259;ng k&yacute; h&agrave;ng ng&agrave;y ch&#7859;ng h&#7841;n), m&#7895;i l&#7847;n th&ecirc;m v&agrave;i &#273;i&#7875;m d&#7919; li&#7879;u m&#7899;i. K&eacute;o theo &#273;&oacute; l&agrave; m&ocirc; h&igrave;nh c&#7911;a ch&uacute;ng ta c&#361;ng ph&#7843;i thay &#273;&#7893;i m&#7897;t ch&uacute;t &#273;&#7875; ph&ugrave; h&#7907;p v&#7899;i c&aacute;c d&#7919; li&#7879;u m&#7899;i n&agrave;y. N&#7871;u l&agrave;m theo Batch Gradient Descent, t&#7913;c t&iacute;nh l&#7841;i &#273;&#7841;o h&agrave;m c&#7911;a h&agrave;m m&#7845;t m&aacute;t t&#7841;i t&#7845;t c&#7843; c&aacute;c &#273;i&#7875;m d&#7919; li&#7879;u, th&igrave; th&#7901;i gian t&iacute;nh to&aacute;n s&#7869; r&#7845;t l&acirc;u, v&agrave; thu&#7853;t to&aacute;n c&#7911;a ch&uacute;ng ta coi nh&#432; kh&ocirc;ng <em>online</em> n&#7919;a do m&#7845;t qu&aacute; nhi&#7873;u th&#7901;i gian t&iacute;nh to&aacute;n.</p>

<p>Tr&ecirc;n th&#7921;c t&#7871;, c&oacute; m&#7897;t thu&#7853;t to&aacute;n &#273;&#417;n gi&#7843;n h&#417;n v&agrave; t&#7887; ra r&#7845;t hi&#7879;u qu&#7843;, c&oacute; t&ecirc;n g&#7885;i l&agrave; Stochastic Gradient Descent (SGD).
<a name="-stochastic-gradient-descent" href="gradientdescent2.html"></a></p>

<h3 id="22-stochastic-gradient-descent">2.2. Stochastic Gradient Descent.</h3>
<p>Trong thu&#7853;t to&aacute;n n&agrave;y, t&#7841;i 1 th&#7901;i &#273;i&#7875;m, ta ch&#7881; t&iacute;nh &#273;&#7841;o h&agrave;m c&#7911;a h&agrave;m m&#7845;t m&aacute;t d&#7921;a tr&ecirc;n <em>ch&#7881; m&#7897;t</em> &#273;i&#7875;m d&#7919; li&#7879;u \(\mathbf{x_i}\) r&#7891;i c&#7853;p nh&#7853;t \(\theta\) d&#7921;a tr&ecirc;n &#273;&#7841;o h&agrave;m n&agrave;y. Vi&#7879;c n&agrave;y &#273;&#432;&#7907;c th&#7921;c hi&#7879;n v&#7899;i t&#7915;ng &#273;i&#7875;m tr&ecirc;n to&agrave;n b&#7897; d&#7919; li&#7879;u, sau &#273;&oacute; l&#7863;p l&#7841;i qu&aacute; tr&igrave;nh tr&ecirc;n. Thu&#7853;t to&aacute;n r&#7845;t &#273;&#417;n gi&#7843;n n&agrave;y tr&ecirc;n th&#7921;c t&#7871; l&#7841;i l&agrave;m vi&#7879;c r&#7845;t hi&#7879;u qu&#7843;.</p>

<p>M&#7895;i l&#7847;n duy&#7879;t m&#7897;t l&#432;&#7907;t qua <em>t&#7845;t c&#7843;</em> c&aacute;c &#273;i&#7875;m tr&ecirc;n to&agrave;n b&#7897; d&#7919; li&#7879;u &#273;&#432;&#7907;c g&#7885;i l&agrave; m&#7897;t epoch. V&#7899;i GD th&ocirc;ng th&#432;&#7901;ng th&igrave; m&#7895;i epoch &#7913;ng v&#7899;i 1 l&#7847;n c&#7853;p nh&#7853;t \(\theta\), v&#7899;i SGD th&igrave; m&#7895;i epoch &#7913;ng v&#7899;i \(N\) l&#7847;n c&#7853;p nh&#7853;t \(\theta\) v&#7899;i \(N\) l&agrave; s&#7889; &#273;i&#7875;m d&#7919; li&#7879;u. Nh&igrave;n v&agrave;o m&#7897;t m&#7863;t, vi&#7879;c c&#7853;p nh&#7853;t t&#7915;ng &#273;i&#7875;m m&#7897;t nh&#432; th&#7871; n&agrave;y c&oacute; th&#7875; l&agrave;m gi&#7843;m &#273;i t&#7889;c &#273;&#7897; th&#7921;c hi&#7879;n 1 epoch. Nh&#432;ng nh&igrave;n v&agrave;o m&#7897;t m&#7863;t kh&aacute;c, SGD ch&#7881; y&ecirc;u c&#7847;u m&#7897;t l&#432;&#7907;ng epoch r&#7845;t nh&#7887; (th&#432;&#7901;ng l&agrave; 10 cho l&#7847;n &#273;&#7847;u ti&ecirc;n, sau &#273;&oacute; khi c&oacute; d&#7919; li&#7879;u m&#7899;i th&igrave; ch&#7881; c&#7847;n ch&#7841;y d&#432;&#7899;i m&#7897;t epoch l&agrave; &#273;&atilde; c&oacute; nghi&#7879;m t&#7889;t). V&igrave; v&#7853;y SGD ph&ugrave; h&#7907;p v&#7899;i c&aacute;c b&agrave;i to&aacute;n c&oacute; l&#432;&#7907;ng c&#417; s&#7903; d&#7919; li&#7879;u l&#7899;n (ch&#7911; y&#7871;u l&agrave; Deep Learning m&agrave; ch&uacute;ng ta s&#7869; th&#7845;y trong ph&#7847;n sau c&#7911;a blog) v&agrave; c&aacute;c b&agrave;i to&aacute;n y&ecirc;u c&#7847;u m&ocirc; h&igrave;nh thay &#273;&#7893;i li&ecirc;n t&#7909;c, t&#7913;c online learning.</p>

<p><strong>Th&#7913; t&#7921; l&#7921;a ch&#7885;n &#273;i&#7875;m d&#7919; li&#7879;u</strong></p>

<p>M&#7897;t &#273;i&#7875;m c&#7847;n l&#432;u &yacute; &#273;&oacute; l&agrave;: sau m&#7895;i epoch, ch&uacute;ng ta c&#7847;n shuffle (x&aacute;o tr&#7897;n) th&#7913; t&#7921; c&#7911;a c&aacute;c d&#7919; li&#7879;u &#273;&#7875; &#273;&#7843;m b&#7843;o t&iacute;nh ng&#7851;u nhi&ecirc;n. Vi&#7879;c n&agrave;y c&#361;ng &#7843;nh h&#432;&#7903;ng t&#7899;i hi&#7879;u n&#259;ng c&#7911;a SGD.</p>

<p>M&#7897;t c&aacute;ch to&aacute;n h&#7885;c, quy t&#7855;c c&#7853;p nh&#7853;t c&#7911;a SGD l&agrave;:
\[
\theta = \theta - \eta \nabla_{\theta} J(\theta; \mathbf{x}_i; \mathbf{y}_i)
\]</p>

<p>trong &#273;&oacute; \(J(\theta; \mathbf{x}_i; \mathbf{y}_i)\) l&agrave; h&agrave;m m&#7845;t m&aacute;t v&#7899;i ch&#7881; 1 c&#7863;p &#273;i&#7875;m d&#7919; li&#7879;u (input, label) l&agrave; (\(\mathbf{x}_i, \mathbf{y}_i\)). <strong>Ch&uacute; &yacute;:</strong> ch&uacute;ng ta ho&agrave;n to&agrave;n c&oacute; th&#7875; &aacute;p d&#7909;ng c&aacute;c thu&#7853;t to&aacute;n t&#259;ng t&#7889;c GD nh&#432; Momentum, AdaGrad,&hellip; v&agrave;o SGD.</p>

<p><a name="vi-du-voi-bai-toan-linear-regression" href="gradientdescent2.html"></a></p>

<h4 id="v&iacute;-d&#7909;-v&#7899;i-b&agrave;i-to&aacute;n-linear-regression">V&iacute; d&#7909; v&#7899;i b&agrave;i to&aacute;n Linear Regression</h4>
<p>V&#7899;i b&agrave;i to&aacute;n Linear Regression, \(\theta = \mathbf{w}\), h&agrave;m m&#7845;t m&aacute;t t&#7841;i m&#7897;t &#273;i&#7875;m d&#7919; li&#7879;u l&agrave;:
\[
J(\mathbf{w}; \mathbf{x}_i; y_i) = \frac{1}{2}(\mathbf{x}_i \mathbf{w} - y_i)^2
\]
&#272;&#7841;o h&agrave;m theo \(\mathbf{w}\) t&#432;&#417;ng &#7913;ng l&agrave;:
\[
\nabla_{\mathbf{w}}J(\mathbf{w}; \mathbf{x}_i; y_i) = \mathbf{x}_i^T(\mathbf{x}_i \mathbf{w} - y_i)
\]
V&agrave; d&#432;&#7899;i &#273;&acirc;y l&agrave; h&agrave;m s&#7889; trong python &#273;&#7875; gi&#7843;i Linear Regression theo SGD:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># single point gradient
</span><span class="k">def</span> <span class="nf">sgrad</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">rd_id</span><span class="p">):</span>
    <span class="n">true_i</span> <span class="o">=</span> <span class="n">rd_id</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">xi</span> <span class="o">=</span> <span class="n">Xbar</span><span class="p">[</span><span class="n">true_i</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">yi</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">true_i</span><span class="p">]</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">yi</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">xi</span><span class="o">*</span><span class="n">a</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">SGD</span><span class="p">(</span><span class="n">w_init</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">eta</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">w_init</span><span class="p">]</span>
    <span class="n">w_last_check</span> <span class="o">=</span> <span class="n">w_init</span>
    <span class="n">iter_check_w</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># shuffle data 
</span>        <span class="n">rd_id</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span> 
            <span class="n">g</span> <span class="o">=</span> <span class="n">sgrad</span><span class="p">(</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">i</span><span class="p">,</span> <span class="n">rd_id</span><span class="p">)</span>
            <span class="n">w_new</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">eta</span><span class="o">*</span><span class="n">g</span>
            <span class="n">w</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">w_new</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">count</span><span class="o">%</span><span class="n">iter_check_w</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">w_this_check</span> <span class="o">=</span> <span class="n">w_new</span>                 
                <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_this_check</span> <span class="o">-</span> <span class="n">w_last_check</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">w_init</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>                                    
                    <span class="k">return</span> <span class="n">w</span>
                <span class="n">w_last_check</span> <span class="o">=</span> <span class="n">w_this_check</span>
    <span class="k">return</span> <span class="n">w</span>
</code></pre></div></div>

<p>K&#7871;t qu&#7843; &#273;&#432;&#7907;c cho nh&#432; h&igrave;nh d&#432;&#7899;i &#273;&acirc;y (<a href="/2017/01/12/gradientdescent/#quay-lai-voi-bai-toan-linear-regression">v&#7899;i d&#7919; li&#7879;u &#273;&#432;&#7907;c t&#7841;o gi&#7889;ng nh&#432; &#7903; ph&#7847;n 1</a>).</p>

<div>
<table width="100%" style="border: 0px solid white"><tr><td width="40%" style="border: 0px solid white"> 
        <img style="display:block;" width="100%" src="images/GD-LR_SGD_contours.gif"></td>
        <td width="40%" style="border: 0px solid white">
        <img style="display:block;" width="100%" src="images/GD-LR_SGD_loss.png"></td>
    </tr></table><div class="thecap"> Tr&aacute;i: &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m v&#7899;i SGD. Ph&#7843;i: gi&aacute; tr&#7883; c&#7911;a loss function t&#7841;i 50 v&ograve;ng l&#7863;p &#273;&#7847;u ti&ecirc;n. </div>
</div>

<p>H&igrave;nh b&ecirc;n tr&aacute;i m&ocirc; t&#7843; &#273;&#432;&#7901;ng &#273;i c&#7911;a nghi&#7879;m. Ch&uacute;ng ta th&#7845;y r&#7857;ng &#273;&#432;&#7901;ng &#273;i kh&aacute; l&agrave; <em>zigzag</em> ch&#7913; kh&ocirc;ng <em>m&#432;&#7907;t</em> nh&#432; khi s&#7917; d&#7909;ng GD. &#272;i&#7873;u n&agrave;y l&agrave; d&#7877; hi&#7875;u v&igrave; m&#7897;t &#273;i&#7875;m d&#7919; li&#7879;u kh&ocirc;ng th&#7875; &#273;&#7841;i di&#7879;n cho to&agrave;n b&#7897; d&#7919; li&#7879;u &#273;&#432;&#7907;c. Tuy nhi&ecirc;n, ch&uacute;ng ta c&#361;ng th&#7845;y r&#7857;ng thu&#7853;t to&aacute;n h&#7897;i t&#7909; kh&aacute; nhanh &#273;&#7871;n v&ugrave;ng l&acirc;n c&#7853;n c&#7911;a nghi&#7879;m. V&#7899;i 1000 &#273;i&#7875;m d&#7919; li&#7879;u, SGD ch&#7881; c&#7847;n g&#7847;n 3 epoches (2911 t&#432;&#417;ng &#7913;ng v&#7899;i 2911 l&#7847;n c&#7853;p nh&#7853;t, m&#7895;i l&#7847;n l&#7845;y 1 &#273;i&#7875;m). N&#7871;u so v&#7899;i con s&#7889; 49 v&ograve;ng l&#7863;p (epoches) nh&#432; k&#7871;t qu&#7843; t&#7889;t nh&#7845;t c&oacute; &#273;&#432;&#7907;c b&#7857;ng GD, th&igrave; k&#7871;t qu&#7843; n&agrave;y l&#7907;i h&#417;n r&#7845;t nhi&#7873;u.</p>

<p>H&igrave;nh b&ecirc;n ph&#7843;i m&ocirc; t&#7843; h&agrave;m m&#7845;t m&aacute;t cho to&agrave;n b&#7897; d&#7919; li&#7879;u sau khi <em>ch&#7881;</em> s&#7917; d&#7909;ng 50 &#273;i&#7875;m d&#7919; li&#7879;u &#273;&#7847;u ti&ecirc;n. M&#7863;c d&ugrave; kh&ocirc;ng <em>m&#432;&#7907;t</em>, t&#7889;c &#273;&#7897; h&#7897;i t&#7909; v&#7851;n r&#7845;t nhanh.</p>

<p><em>Th&#7921;c t&#7871; cho th&#7845;y ch&#7881; l&#7845;y kho&#7843;ng 10 &#273;i&#7875;m l&agrave; ta &#273;&atilde; c&oacute; th&#7875; x&aacute;c &#273;&#7883;nh &#273;&#432;&#7907;c g&#7847;n &#273;&uacute;ng ph&#432;&#417;ng tr&igrave;nh &#273;&#432;&#7901;ng th&#7859;ng c&#7847;n t&igrave;m r&#7891;i. &#272;&acirc;y ch&iacute;nh l&agrave; &#432;u &#273;i&#7875;m c&#7911;a SGD - h&#7897;i t&#7909; r&#7845;t nhanh.</em></p>

<p><a name="-mini-batch-gradient-descent" href="gradientdescent2.html"></a></p>

<h3 id="23-mini-batch-gradient-descent">2.3. Mini-batch Gradient Descent</h3>
<p>Kh&aacute;c v&#7899;i SGD, mini-batch s&#7917; d&#7909;ng m&#7897;t s&#7889; l&#432;&#7907;ng \(n\) l&#7899;n h&#417;n 1 (nh&#432;ng v&#7851;n nh&#7887; h&#417;n t&#7893;ng s&#7889; d&#7919; li&#7879;u \(N\)r&#7845;t nhi&#7873;u). Gi&#7889;ng v&#7899;i SGD, Mini-batch Gradient Descent b&#7855;t &#273;&#7847;u m&#7895;i epoch b&#7857;ng vi&#7879;c x&aacute;o tr&#7897;n ng&#7851;u nhi&ecirc;n d&#7919; li&#7879;u r&#7891;i chia to&agrave;n b&#7897; d&#7919; li&#7879;u th&agrave;nh c&aacute;c <em>mini-batch</em>, m&#7895;i <em>mini-batch</em> c&oacute; \(n\) &#273;i&#7875;m d&#7919; li&#7879;u (tr&#7915; mini-batch cu&#7889;i c&oacute; th&#7875; c&oacute; &iacute;t h&#417;n n&#7871;u \(N\) kh&ocirc;ng chia h&#7871;t cho \(n\)). M&#7895;i l&#7847;n c&#7853;p nh&#7853;t, thu&#7853;t to&aacute;n n&agrave;y l&#7845;y ra m&#7897;t mini-batch &#273;&#7875; t&iacute;nh to&aacute;n &#273;&#7841;o h&agrave;m r&#7891;i c&#7853;p nh&#7853;t. C&ocirc;ng th&#7913;c c&oacute; th&#7875; vi&#7871;t d&#432;&#7899;i d&#7841;ng:
\[
\theta = \theta - \eta\nabla_{\theta} J(\theta; \mathbf{x}_{i:i+n}; \mathbf{y}_{i:i+n})
\]
V&#7899;i \(\mathbf{x}_{i:i+n}\) &#273;&#432;&#7907;c hi&#7875;u l&agrave; d&#7919; li&#7879;u t&#7915; th&#7913; \(i\) t&#7899;i th&#7913; \(i+n-1\) (theo k&yacute; hi&#7879;u c&#7911;a Python). D&#7919; li&#7879;u n&agrave;y sau m&#7895;i epoch l&agrave; kh&aacute;c nhau v&igrave; ch&uacute;ng c&#7847;n &#273;&#432;&#7907;c x&aacute;o tr&#7897;n. M&#7897;t l&#7847;n n&#7919;a, c&aacute;c thu&#7853;t to&aacute;n kh&aacute;c cho GD nh&#432; Momentum, Adagrad, Adadelta,&hellip; c&#361;ng c&oacute; th&#7875; &#273;&#432;&#7907;c &aacute;p d&#7909;ng v&agrave;o &#273;&acirc;y.</p>

<p>Mini-batch GD &#273;&#432;&#7907;c s&#7917; d&#7909;ng trong h&#7847;u h&#7871;t c&aacute;c thu&#7853;t to&aacute;n Machine Learning, &#273;&#7863;c bi&#7879;t l&agrave; trong Deep Learning. Gi&aacute; tr&#7883; \(n\) th&#432;&#7901;ng &#273;&#432;&#7907;c ch&#7885;n l&agrave; kho&#7843;ng t&#7915; 50 &#273;&#7871;n 100.</p>

<p>D&#432;&#7899;i &#273;&acirc;y l&agrave; v&iacute; d&#7909; v&#7873; gi&aacute; tr&#7883; c&#7911;a h&agrave;m m&#7845;t m&aacute;t m&#7895;i khi c&#7853;p nh&#7853;t tham s&#7889; \(\theta\) c&#7911;a m&#7897;t b&agrave;i to&aacute;n kh&aacute;c ph&#7913;c t&#7841;p h&#417;n.</p>

<div class="imgcap">
 <img src="images/f3-Stogra.png" align="center" width="400"><div class="thecap"> H&agrave;m m&#7845;t m&aacute;t <em>nh&#7843;y l&ecirc;n nh&#7843;y xu&#7889;ng</em> (fluctuate) sau m&#7895;i l&#7847;n c&#7853;p nh&#7853;t nh&#432;ng nh&igrave;n chung gi&#7843;m d&#7847;n v&agrave; c&oacute; xu h&#432;&#7899;ng h&#7897;i t&#7909; v&#7873; cu&#7889;i. (Ngu&#7891;n: <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Wikipedia</a>). </div>
</div>

<p>&#272;&#7875; c&oacute; th&ecirc;m th&ocirc;ng tin chi ti&#7871;t h&#417;n, b&#7841;n &#273;&#7885;c c&oacute; th&#7875; t&igrave;m trong <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">b&agrave;i vi&#7871;t r&#7845;t t&#7889;t n&agrave;y</a>.</p>

<p><a name="-stopping-criteria-dieu-kien-dung" href="gradientdescent2.html"></a></p>

<h2 id="3-stopping-criteria-&#273;i&#7873;u-ki&#7879;n-d&#7915;ng">3. Stopping Criteria (&#273;i&#7873;u ki&#7879;n d&#7915;ng)</h2>

<p>C&oacute; m&#7897;t &#273;i&#7875;m c&#361;ng quan tr&#7885;ng m&agrave; t&#7915; &#273;&#7847;u t&ocirc;i ch&#432;a nh&#7855;c &#273;&#7871;n: khi n&agrave;o th&igrave; ch&uacute;ng ta bi&#7871;t thu&#7853;t to&aacute;n &#273;&atilde; h&#7897;i t&#7909; v&agrave; d&#7915;ng l&#7841;i?</p>

<p>Trong th&#7921;c nghi&#7879;m, c&oacute; m&#7897;t v&agrave;i ph&#432;&#417;ng ph&aacute;p nh&#432; d&#432;&#7899;i &#273;&acirc;y:</p>

<ol><li>Gi&#7899;i h&#7841;n s&#7889; v&ograve;ng l&#7863;p: &#273;&acirc;y l&agrave; ph&#432;&#417;ng ph&aacute;p ph&#7893; bi&#7871;n nh&#7845;t v&agrave; c&#361;ng &#273;&#7875; &#273;&#7843;m b&#7843;o r&#7857;ng ch&#432;&#417;ng tr&igrave;nh ch&#7841;y kh&ocirc;ng qu&aacute; l&acirc;u. Tuy nhi&ecirc;n, m&#7897;t nh&#432;&#7907;c &#273;i&#7875;m c&#7911;a c&aacute;ch l&agrave;m n&agrave;y l&agrave; c&oacute; th&#7875; thu&#7853;t to&aacute;n d&#7915;ng l&#7841;i tr&#432;&#7899;c khi &#273;&#7911; g&#7847;n v&#7899;i nghi&#7879;m.</li>
  <li>So s&aacute;nh gradient c&#7911;a nghi&#7879;m t&#7841;i hai l&#7847;n c&#7853;p nh&#7853;t li&ecirc;n ti&#7871;p, khi n&agrave;o gi&aacute; tr&#7883; n&agrave;y &#273;&#7911; nh&#7887; th&igrave; d&#7915;ng l&#7841;i. Ph&#432;&#417;ng ph&aacute;p n&agrave;y c&#361;ng c&oacute; m&#7897;t nh&#432;&#7907;c &#273;i&#7875;m l&#7899;n l&agrave; vi&#7879;c t&iacute;nh &#273;&#7841;o h&agrave;m &#273;&ocirc;i khi tr&#7903; n&ecirc;n qu&aacute; ph&#7913;c t&#7841;p (v&iacute; d&#7909; nh&#432; khi c&oacute; qu&aacute; nhi&#7873;u d&#7919; li&#7879;u), n&#7871;u &aacute;p d&#7909;ng ph&#432;&#417;ng ph&aacute;p n&agrave;y th&igrave; coi nh&#432; ta kh&ocirc;ng &#273;&#432;&#7907;c l&#7907;i khi s&#7917; d&#7909;ng SGD v&agrave; mini-batch GD.</li>
  <li>So s&aacute;nh gi&aacute; tr&#7883; c&#7911;a h&agrave;m m&#7845;t m&aacute;t c&#7911;a nghi&#7879;m t&#7841;i hai l&#7847;n c&#7853;p nh&#7853;t li&ecirc;n ti&#7871;p, khi n&agrave;o gi&aacute; tr&#7883; n&agrave;y &#273;&#7911; nh&#7887; th&igrave; d&#7915;ng l&#7841;i. Nh&#432;&#7907;c &#273;i&#7875;m c&#7911;a ph&#432;&#417;ng ph&aacute;p n&agrave;y l&agrave; n&#7871;u t&#7841;i m&#7897;t th&#7901;i &#273;i&#7875;m, &#273;&#7891; th&#7883; h&agrave;m s&#7889; c&oacute; d&#7841;ng <em>b&#7859;ng ph&#7859;ng</em> t&#7841;i m&#7897;t khu v&#7921;c nh&#432;ng khu v&#7921;c &#273;&oacute; kh&ocirc;ng ch&#7913;a &#273;i&#7875;m local minimum (khu v&#7921;c n&agrave;y th&#432;&#7901;ng &#273;&#432;&#7907;c g&#7885;i l&agrave; saddle points), thu&#7853;t to&aacute;n c&#361;ng d&#7915;ng l&#7841;i tr&#432;&#7899;c khi &#273;&#7841;t gi&aacute; tr&#7883; mong mu&#7889;n.</li>
  <li>Trong SGD v&agrave; mini-batch GD, c&aacute;ch th&#432;&#7901;ng d&ugrave;ng l&agrave; so s&aacute;nh nghi&#7879;m sau m&#7897;t v&agrave;i l&#7847;n c&#7853;p nh&#7853;t. Trong &#273;o&#7841;n code Python ph&iacute;a tr&ecirc;n v&#7873; SGD, t&ocirc;i &aacute;p d&#7909;ng vi&#7879;c so s&aacute;nh n&agrave;y m&#7895;i khi nghi&#7879;m &#273;&#432;&#7907;c c&#7853;p nh&#7853;t 10 l&#7847;n. Vi&#7879;c l&agrave;m n&agrave;y c&#361;ng t&#7887; ra kh&aacute; hi&#7879;u qu&#7843;.</li>
</ol><p><a name="-mot-phuong-phap-toi-uu-don-gian-khac-newtons-method" href="gradientdescent2.html"></a></p>

<h2 id="4-m&#7897;t-ph&#432;&#417;ng-ph&aacute;p-t&#7889;i-&#432;u-&#273;&#417;n-gi&#7843;n-kh&aacute;c-newtons-method">4. M&#7897;t ph&#432;&#417;ng ph&aacute;p t&#7889;i &#432;u &#273;&#417;n gi&#7843;n kh&aacute;c: Newton&rsquo;s method</h2>

<p>Nh&acirc;n ti&#7879;n &#273;ang n&oacute;i v&#7873; t&#7889;i &#432;u, t&ocirc;i xin gi&#7899;i thi&#7879;u m&#7897;t ph&#432;&#417;ng ph&aacute;p n&#7919;a c&oacute; c&aacute;ch gi&#7843;i th&iacute;ch &#273;&#417;n gi&#7843;n: Newton&rsquo;s method. C&aacute;c ph&#432;&#417;ng ph&aacute;p GD t&ocirc;i &#273;&atilde; tr&igrave;nh b&agrave;y c&ograve;n &#273;&#432;&#7907;c g&#7885;i l&agrave; first-order methods, v&igrave; l&#7901;i gi&#7843;i t&igrave;m &#273;&#432;&#7907;c d&#7921;a tr&ecirc;n &#273;&#7841;o h&agrave;m b&#7853;c nh&#7845;t c&#7911;a h&agrave;m s&#7889;. Newton&rsquo;s method l&agrave; m&#7897;t second-order method, t&#7913;c l&#7901;i gi&#7843;i y&ecirc;u c&#7847;u t&iacute;nh &#273;&#7871;n &#273;&#7841;o h&agrave;m b&#7853;c hai.</p>

<p>Nh&#7855;c l&#7841;i r&#7857;ng, cho t&#7899;i th&#7901;i &#273;i&#7875;m n&agrave;y, ch&uacute;ng ta lu&ocirc;n gi&#7843;i ph&#432;&#417;ng tr&igrave;nh &#273;&#7841;o h&agrave;m c&#7911;a h&agrave;m m&#7845;t m&aacute;t b&#7857;ng 0 &#273;&#7875; t&igrave;m c&aacute;c &#273;i&#7875;m local minimun. (V&agrave; trong nhi&#7873;u tr&#432;&#7901;ng h&#7907;p, coi nghi&#7879;m t&igrave;m &#273;&#432;&#7907;c l&agrave; nghi&#7879;m c&#7911;a b&agrave;i to&aacute;n t&igrave;m gi&aacute; tr&#7883; nh&#7887; nh&#7845;t c&#7911;a h&agrave;m m&#7845;t m&aacute;t). C&oacute; m&#7897;t thu&#7853;t to&aacute;n n&#7889;i ti&#7871;ng gi&uacute;p gi&#7843;i b&agrave;i to&aacute;n \(f(x) = 0\), thu&#7853;t to&aacute;n &#273;&oacute; c&oacute; t&ecirc;n l&agrave; Newton&rsquo;s method.</p>

<p><a name="newtons-method-cho-giai-phuong-trinh-%5C%5Cfx--%5C%5C" href="gradientdescent2.html"></a></p>

<h3 id="newtons-method-cho-gi&#7843;i-ph&#432;&#417;ng-tr&igrave;nh-fx--0">Newton&rsquo;s method cho gi&#7843;i ph&#432;&#417;ng tr&igrave;nh \(f(x) = 0\)</h3>

<p>Thu&#7853;t to&aacute;n Newton&rsquo;s method &#273;&#432;&#7907;c m&ocirc; t&#7843; trong h&igrave;nh &#273;&#7897;ng minh h&#7885;a d&#432;&#7899;i &#273;&acirc;y:</p>

<div class="imgcap">
 <img src="images/e0-NewtonIteration_Ani.gif" align="center" width="500"><div class="thecap"> H&igrave;nh 3: Minh h&#7885;a thu&#7853;t to&aacute;n Newton's method trong gi&#7843;i ph&#432;&#417;ng tr&igrave;nh. (  Ngu&#7891;n: <a href="https://en.wikipedia.org/wiki/Newton's_method"> Newton's method - Wikipedia</a>).</div>
</div>

<p>&Yacute; t&#432;&#7903;ng gi&#7843;i b&agrave;i to&aacute;n \(f(x) = 0\) b&#7857;ng ph&#432;&#417;ng ph&aacute;p Newton&rsquo;s method nh&#432; sau. Xu&#7845;t ph&aacute;t t&#7915; m&#7897;t &#273;i&#7875;m \(x_0\) &#273;&#432;&#7907;c cho l&agrave; g&#7847;n v&#7899;i nghi&#7879;m \(x^*\). Sau &#273;&oacute; v&#7869; &#273;&#432;&#7901;ng ti&#7871;p tuy&#7871;n (m&#7863;t ti&#7871;p tuy&#7871;n trong kh&ocirc;ng gian nhi&#7873;u chi&#7873;u) v&#7899;i &#273;&#7891; th&#7883; h&agrave;m s&#7889; \(y = f(x)\) t&#7841;i &#273;i&#7875;m tr&ecirc;n &#273;&#7891; th&#7883; c&oacute; ho&agrave;nh &#273;&#7897; \(x_0\). Giao &#273;i&#7875;m \(x_1\) c&#7911;a &#273;&#432;&#7901;ng ti&#7871;p tuy&#7871;n n&agrave;y v&#7899;i tr&#7909;c ho&agrave;nh &#273;&#432;&#7907;c xem l&agrave; g&#7847;n v&#7899;i nghi&#7879;m \(x^*\) h&#417;n. Thu&#7853;t to&aacute;n l&#7863;p l&#7841;i v&#7899;i &#273;i&#7875;m m&#7899;i \(x_1\) v&agrave; c&#7913; nh&#432; v&#7853;y &#273;&#7871;n khi ta &#273;&#432;&#7907;c \(f(x_t) \approx 0\).</p>

<p>&#272;&oacute; l&agrave; &yacute; ngh&#297;a h&igrave;nh h&#7885;c c&#7911;a Newton&rsquo;s method, ch&uacute;ng ta c&#7847;n m&#7897;t c&ocirc;ng th&#7913;c &#273;&#7875; c&oacute; th&#7875; d&#7921;a v&agrave;o &#273;&oacute; &#273;&#7875; l&#7853;p tr&igrave;nh. Vi&#7879;c n&agrave;y kh&ocirc;ng qu&aacute; ph&#7913;c t&#7841;p v&#7899;i c&aacute;c b&#7841;n thi &#273;&#7841;i h&#7885;c m&ocirc;n to&aacute;n &#7903; VN. Th&#7853;t v&#7853;y, ph&#432;&#417;ng tr&igrave;nh ti&#7871;p tuy&#7871;n v&#7899;i &#273;&#7891; th&#7883; c&#7911;a h&agrave;m \(f(x)\) t&#7841;i &#273;i&#7875;m c&oacute; ho&agrave;nh &#273;&#7897; \(x_t\) l&agrave;:
\[
y = f&rsquo;(x_t)(x - x_t) + f(x_t)
\]
Giao &#273;i&#7875;m c&#7911;a &#273;&#432;&#7901;ng th&#7859;ng n&agrave;y v&#7899;i tr&#7909;c \(x\) t&igrave;m &#273;&#432;&#7907;c b&#7857;ng c&aacute;ch gi&#7843;i ph&#432;&#417;ng tr&igrave;nh v&#7871; ph&#7843;i c&#7911;a bi&#7875;u th&#7913;c tr&ecirc;n b&#7857;ng 0, t&#7913;c l&agrave;:
\[
x = x_t - \frac{f(x_t)}{f&rsquo;(x_t)} \triangleq x_{t+1}
\]</p>

<p><a name="newtons-method-trong-bai-toan-tim-local-minimun" href="gradientdescent2.html"></a></p>

<h3 id="newtons-method-trong-b&agrave;i-to&aacute;n-t&igrave;m-local-minimun">Newton&rsquo;s method trong b&agrave;i to&aacute;n t&igrave;m local minimun</h3>
<p>&Aacute;p d&#7909;ng ph&#432;&#417;ng ph&aacute;p n&agrave;y cho vi&#7879;c gi&#7843;i ph&#432;&#417;ng tr&igrave;nh \(f&rsquo;(x) = 0\) ta c&oacute;:
\[
x_{t+1} = x_t -(f&rdquo;(x_t))^{-1}{f&rsquo;(x_t)}
\]</p>

<p>V&agrave; trong kh&ocirc;ng gian nhi&#7873;u chi&#7873;u v&#7899;i \(\theta\) l&agrave; bi&#7871;n:
\[
\theta = \theta - \mathbf{H}(J(\theta))^{-1} \nabla_{\theta} J(\theta)
\]
trong &#273;&oacute; \(\mathbf{H}(J(\theta))\) l&agrave; &#273;&#7841;o h&agrave;m b&#7853;c hai c&#7911;a h&agrave;m m&#7845;t m&#7845;t (c&ograve;n g&#7885;i l&agrave; <a href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian matrix</a>). Bi&#7875;u th&#7913;c n&agrave;y l&agrave; m&#7897;t ma tr&#7853;n n&#7871;u \(\theta\) l&agrave; m&#7897;t vector. V&agrave; \(\mathbf{H}(J(\theta))^{-1}\) ch&iacute;nh l&agrave; ngh&#7883;ch &#273;&#7843;o c&#7911;a ma tr&#7853;n &#273;&oacute;.</p>

<p><a name="han-che-cua-newtons-method" href="gradientdescent2.html"></a></p>

<h3 id="h&#7841;n-ch&#7871;-c&#7911;a-newtons-method">H&#7841;n ch&#7871; c&#7911;a Newton&rsquo;s method</h3>
<ul><li>&#272;i&#7875;m kh&#7903;i t&#7841;o ph&#7843;i <em>r&#7845;t</em> g&#7847;n v&#7899;i nghi&#7879;m \(x^*\).
&Yacute; t&#432;&#7903;ng s&acirc;u xa h&#417;n c&#7911;a Newton&rsquo;s method l&agrave; d&#7921;a tr&ecirc;n khai tri&#7875;n Taylor c&#7911;a h&agrave;m s&#7889; \(f(x)\) t&#7899;i &#273;&#7841;o h&agrave;m th&#7913; nh&#7845;t:
\[
0 = f(x^*) \approx f(x_t) + f&rsquo;(x_t)(x_t - x^*)
\]
T&#7915; &#273;&oacute; suy ra: \(x^* \approx x_t - \frac{f(x_t)}{f&rsquo;(x_t)}\). 
M&#7897;t &#273;i&#7875;m r&#7845;t quan tr&#7885;ng, khai tri&#7875;n Taylor ch&#7881; &#273;&uacute;ng n&#7871;u \(x_t\) r&#7845;t g&#7847;n v&#7899;i \(x^*\)!
D&#432;&#7899;i &#273;&acirc;y l&agrave; m&#7897;t v&iacute; d&#7909; kinh &#273;i&#7875;n tr&ecirc;n Wikipedia v&#7873; vi&#7879;c Newton&rsquo;s method cho m&#7897;t d&atilde;y s&#7889; ph&acirc;n k&#7923; (divergence).</li>
</ul><div class="imgcap">
 <img src="images/NewtonsMethodConvergenceFailure.svg-300px-NewtonsMethodConvergenceFailure.svg.png" align="center" width="400"><div class="thecap"> H&igrave;nh 4: Nghi&#7879;m l&agrave; m&#7897;t &#273;i&#7875;m g&#7847;n -2. Ti&#7871;p tuy&#7871;n c&#7911;a &#273;&#7891; th&#7883; h&agrave;m s&#7889; t&#7841;i &#273;i&#7875;m c&oacute; ho&agrave;nh &#273;&#7897; b&#7857;ng 0 c&#7855;t tr&#7909;c ho&agrave;nh t&#7841;i 1, v&agrave; ng&#432;&#7907;c l&#7841;i. Trong tr&#432;&#7901;ng h&#7907;p n&agrave;y, Newton's method kh&ocirc;ng bao gi&#7901; h&#7897;i t&#7909;. (Ngu&#7891;n: <a href="https://en.wikipedia.org/wiki/Newton's_method">Wikipedia</a>). </div>
</div>

<ul><li>
    <p>Nh&#7853;n th&#7845;y r&#7857;ng trong vi&#7879;c gi&#7843;i ph&#432;&#417;ng tr&igrave;nh \(f(x) = 0\), ch&uacute;ng ta c&oacute; &#273;&#7841;o h&agrave;m &#7903; m&#7851;u s&#7889;. Khi &#273;&#7841;o h&agrave;m n&agrave;y g&#7847;n v&#7899;i 0, ta s&#7869; &#273;&#432;&#7907;c m&#7897;t &#273;&#432;&#7901;ng th&#7857;ng song song ho&#7863;c g&#7847;n song song v&#7899;i tr&#7909;c ho&agrave;nh. Ta s&#7869; ho&#7863;c kh&ocirc;ng t&igrave;m &#273;&#432;&#7907;c giao &#273;i&#7875;m, ho&#7863;c &#273;&#432;&#7907;c m&#7897;t giao &#273;i&#7875;m &#7903; v&ocirc; c&ugrave;ng. &#272;&#7863;c bi&#7879;t, khi nghi&#7879;m ch&iacute;nh l&agrave; &#273;i&#7875;m c&oacute; &#273;&#7841;o h&agrave;m b&#7857;ng 0, thu&#7853;t to&aacute;n g&#7847;n nh&#432; s&#7869; kh&ocirc;ng t&igrave;m &#273;&#432;&#7907;c nghi&#7879;m!</p>
  </li>
  <li>
    <p>Khi &aacute;p d&#7909;ng Newton&rsquo;s method cho b&agrave;i to&aacute;n t&#7889;i &#432;u trong kh&ocirc;ng gian nhi&#7873;u chi&#7873;u, ch&uacute;ng ta c&#7847;n t&iacute;nh ngh&#7883;ch &#273;&#7843;o c&#7911;a Hessian matrix. Khi s&#7889; chi&#7873;u v&agrave; s&#7889; &#273;i&#7875;m d&#7919; li&#7879;u l&#7899;n, &#273;&#7841;o h&agrave;m b&#7853;c hai c&#7911;a h&agrave;m m&#7845;t m&aacute;t s&#7869; l&agrave; m&#7897;t ma tr&#7853;n r&#7845;t l&#7899;n, &#7843;nh h&#432;&#7903;ng t&#7899;i c&#7843; memory v&agrave; t&#7889;c &#273;&#7897; t&iacute;nh to&aacute;n c&#7911;a h&#7879; th&#7889;ng.</p>
  </li>
</ul><p><a name="-ket-luan" href="gradientdescent2.html"></a></p>

<h2 id="5-k&#7871;t-lu&#7853;n">5. K&#7871;t lu&#7853;n</h2>
<p>Qua hai b&agrave;i vi&#7871;t v&#7873; Gradient Descent n&agrave;y, t&ocirc;i hy v&#7885;ng c&aacute;c b&#7841;n &#273;&atilde; hi&#7875;u v&agrave; l&agrave;m quen v&#7899;i m&#7897;t thu&#7853;t to&aacute;n t&#7889;i &#432;u &#273;&#432;&#7907;c s&#7917; d&#7909;ng nhi&#7873;u nh&#7845;t trong Machine Learning v&agrave; &#273;&#7863;c bi&#7879;t l&agrave; Deep Learning. C&ograve;n nhi&#7873;u bi&#7871;n th&#7875; kh&aacute;c kh&aacute; th&uacute; v&#7883; v&#7873; GD (m&agrave; r&#7845;t c&oacute; th&#7875; t&ocirc;i ch&#432;a bi&#7871;t t&#7899;i), nh&#432;ng t&ocirc;i xin ph&eacute;p &#273;&#432;&#7907;c d&#7915;ng chu&#7895;i b&agrave;i v&#7873; GD t&#7841;i &#273;&acirc;y v&agrave; ti&#7871;p t&#7909;c chuy&#7875;n sang c&aacute;c thu&#7853;t to&aacute;n th&uacute; v&#7883; kh&aacute;c.</p>

<p>Hy v&#7885;ng b&agrave;i vi&#7871;t c&oacute; &iacute;ch v&#7899;i c&aacute;c b&#7841;n.</p>

<p><a name="-tai-lieu-tham-khao" href="gradientdescent2.html"></a></p>

<h2 id="6-t&agrave;i-li&#7879;u-tham-kh&#7843;o">6. T&agrave;i li&#7879;u tham kh&#7843;o</h2>

<p>[1] <a href="https://en.wikipedia.org/wiki/Newton's_method">Newton&rsquo;s method - Wikipedia</a></p>

<p>[2] <a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#stochasticgradientdescent">An overview of gradient descent optimization algorithms</a></p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent - Wikipedia</a></p>

<p>[4] <a href="https://www.youtube.com/watch?v=UfNU3Vhv5CA">Stochastic Gradient Descen - Andrew Ng</a></p>

<p>[5] Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543&ndash; 547.</p>

</div>

<hr><em>N&#7871;u c&oacute; c&acirc;u h&#7887;i, B&#7841;n c&oacute; th&#7875; &#273;&#7875; l&#7841;i comment b&ecirc;n d&#432;&#7899;i ho&#7863;c tr&ecirc;n <a href="https://www.facebook.com/groups/257768141347267/">Forum</a> &#273;&#7875; nh&#7853;n &#273;&#432;&#7907;c c&acirc;u tr&#7843; l&#7901;i s&#7899;m h&#417;n.</em>
<br><em>B&#7841;n &#273;&#7885;c c&oacute; th&#7875; &#7911;ng h&#7897; blog qua <a href="buymeacoffee.html">'Buy me a cofee'</a> &#7903; g&oacute;c tr&ecirc;n b&ecirc;n tr&aacute;i c&#7911;a blog.
</em>

<br><em>T&ocirc;i v&#7915;a ho&agrave;n th&agrave;nh cu&#7889;n ebook 'Machine Learning c&#417; b&#7843;n', b&#7841;n c&oacute; th&#7875; &#273;&#7863;t s&aacute;ch <a href="ebook.html">t&#7841;i &#273;&acirc;y</a>.

C&#7843;m &#417;n b&#7841;n.</em>

<hr><!-- previous and next posts --><div class="PageNavigation">
   
      <a class="prev" style="color: #204081;" href="gradientdescent.html">&laquo; B&agrave;i 7: Gradient Descent (ph&#7847;n 1/2)</a>
   
   
      <a class="next" style="float: right; color: #204081;" href="perceptron.html">B&agrave;i 9: Perceptron Learning Algorithm &raquo;</a>
   
</div>


<!-- disqus comments -->

      <hr><div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname  = 'tiepvu';
  var disqus_identifier = 'tiepvupsu.github.io' + '/2017/01/16/gradientdescent2/';

  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


    <script id="dsq-count-scr" src="js/count.js" async></script><!-- Start of StatCounter Code for Default Guide --><script type="text/javascript">
var sc_project=11221352; 
var sc_invisible=0; 
var sc_security="b7d744c5"; 
var sc_text=2; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("Total visits: <sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'> </"+"script>");
</script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="images/b7d744c5-0" alt="web
analytics"></a> </div></noscript>
<!-- End of StatCounter Code for Default Guide -->

<!-- <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->

<script type="text/javascript" async src="js/2.7.1-MathJax.js">
</script><!-- 
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?...">
</script> --></div>
	        <div class="col-md-2 hidden-xs hidden-sm">
	        	
          <!-- Google search -->
<!--           <table border="0">
          <div id = "top-widget" style="width: 252px; margin-left: -13.5px; margin-top: -10px; margin-bottom: -15px;">
         <script>
           (function() {
             var cx = '012053542614118746585:ktgei4l2oek';
             var gcse = document.createElement('script');
             gcse.type = 'text/javascript';
             gcse.async = true;
             gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
             var s = document.getElementsByTagName('script')[0];
             s.parentNode.insertBefore(gcse, s);
           })();
         </script>
         <gcse:search></gcse:search>
          </div>
          </table> -->

          

         <!--  
          <nav>
          
            <div class="header">Latest by category</div>
            <ul>
              
                
              
                
              
                
              
                
              
                
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/04/02/duality/">B&agrave;i 18: Duality</a></li>
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/03/19/convexopt/">B&agrave;i 17: Convex Optimization Problems</a></li>
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/03/12/convexity/">B&agrave;i 16: Convex sets v&agrave; convex functions</a></li>
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/01/16/gradientdescent2/">B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)</a></li>
                  
                    <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif; color: #204081;" href="/2017/01/12/gradientdescent/">B&agrave;i 7: Gradient Descent (ph&#7847;n 1/2)</a></li>
                  
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
                
              
            </ul>
          </nav>
          



          <nav>
            <div class="header">Latest</div>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/lifesofar2/">Con &#273;&#432;&#7901;ng h&#7885;c PhD c&#7911;a t&ocirc;i</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/10/03/conv2d">B&agrave;i 37: T&iacute;ch ch&#7853;p hai chi&#7873;u</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/09/11/forum/">Gi&#7899;i thi&#7879;u Di&#7877;n &#273;&agrave;n Machine Learning c&#417; b&#7843;n</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/07/06/deeplearning/">B&agrave;i 36. Gi&#7899;i thi&#7879;u v&#7873; Keras</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/06/22/deeplearning/">B&agrave;i 35: L&#432;&#7907;c s&#7917; Deep Learning</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/03/22/phuonghoagiang/">B&#7841;n &#273;&#7885;c vi&#7871;t: Con &#273;&#432;&#7901;ng h&#7885;c Khoa h&#7885;c d&#7919; li&#7879;u c&#7911;a m&#7897;t sinh vi&ecirc;n Kinh t&#7871;</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2018/01/14/id3/">B&agrave;i 34: Decision Trees (1): Iterative Dichotomiser 3</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/31/evaluation/">B&agrave;i 33: C&aacute;c ph&#432;&#417;ng ph&aacute;p &#273;&aacute;nh gi&aacute; m&#7897;t h&#7879; th&#7889;ng ph&acirc;n l&#7899;p</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/20/fundaml_vectors/">FundaML 3: L&agrave;m vi&#7879;c v&#7899;i c&aacute;c m&#7843;ng ng&#7851;u nhi&ecirc;n</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/20/fundaml_matrices/">FundaML 2: L&agrave;m vi&#7879;c v&#7899;i ma tr&#7853;n</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/10/12/fundaml_vectors/">FundaML 1: L&agrave;m vi&#7879;c v&#7899;i m&#7843;ng m&#7897;t chi&#7873;u</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/09/24/fundaml/">Gi&#7899;i thi&#7879;u trang web FundaML.com</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/08/nbc/">B&agrave;i 32: Naive Bayes Classifier</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/08/05/phdlife/">PhD life 1: Qu&aacute; tr&igrave;nh vi&#7871;t v&agrave; nh&#7853;n x&eacute;t c&aacute;c b&agrave;i b&aacute;o khoa h&#7885;c</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/17/mlemap/">B&agrave;i 31: Maximum Likelihood v&agrave; Maximum A Posteriori estimation</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/lifesofar/">Con &#273;&#432;&#7901;ng h&#7885;c To&aacute;n c&#7911;a t&ocirc;i</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/09/prob/">B&agrave;i 30: &Ocirc;n t&#7853;p X&aacute;c Su&#7845;t cho Machine Learning</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/07/02/tl/">Quick Note 2: Transfer Learning cho b&agrave;i to&aacute;n ph&acirc;n lo&#7841;i &#7843;nh</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/06/30/lda/">B&agrave;i 29: Linear Discriminant Analysis</a></li>
              
                <li><a style="text-align: left; font-family: 'Open Sans Condensed', sans-serif;color: #204081"  href="/2017/06/22/qns1/">Quick Notes 1</a></li>
              
            </ul>
          </nav> -->

          <aside class="social"><div class="header">Share</div>
          <div class="share-page">
    <!-- <b>Share this on:</b>  <br> -->

    <!-- Facebook -->
    <!-- <a href="https://facebook.com/sharer/sharer.php?u=https://machinelearningcoban.com/2017/01/16/gradientdescent2/" rel="nofollow" target="_blank" title="Share on Facebook"><img src = "/assets/images/facebook.png" width="25"></a> -->

    <div class="fb-share-button" data-href="https://machinelearningcoban.com/2017/01/16/gradientdescent2/" data-layout="button_count" data-size="small" data-mobile-iframe="true"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://facebook.com/sharer/sharer.php?u=https://machinelearningcoban.com/2017/01/16/gradientdescent2/">Share</a></div>


    <!-- Twitter -->
    <!-- <a href="https://twitter.com/intent/tweet?text=B&agrave;i 8: Gradient Descent (ph&#7847;n 2/2)&url=https://machinelearningcoban.com/2017/01/16/gradientdescent2/&via=&related=" rel="nofollow" target="_blank" title="Share on Twitter" width="25" ><img src = "/assets/images/twitter.png" width="25"></a> -->

    <!-- Google -->
    <!-- <a href="https://plus.google.com/share?url=https://machinelearningcoban.com/2017/01/16/gradientdescent2/" rel="nofollow" target="_blank" title="Share on Google+"><img src = "/assets/images/google.png" width="25"></a> -->

    
    <!-- LinkedIn -->
    <!-- <a href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://machinelearningcoban.com/2017/01/16/gradientdescent2/" target="_blank"> <img src="/assets/images/linkedin.png" alt="LinkedIn" width="25"/> -->
    <!-- </a> -->

    <!-- Email -->
    <a href="/cdn-cgi/l/email-protection#cbf498bea9a1aea8bff698a2a6bba7aeeb98a3aab9aeeb89bebfbfa4a5b8edaaa6bbf089a4afb2f682eef9fbb8aabceef9fbbfa3a2b8eef9fbaaa5afeef9fbbfa3a4beaca3bfeef9fba4adeef9fbb2a4beeaeef9fbeba3bfbfbbb8f1e4e4a6aaa8a3a2a5aea7aeaab9a5a2a5aca8a4a9aaa5e5a8a4a6e4f9fbfafce4fbfae4fafde4acb9aaafa2aea5bfafaeb8a8aea5bff9e4">
        <img src="images/images-email.png" alt="Email" width="25"></a>
    <!-- Print -->
    <a href="javascript:;.html" onclick="window.print()">
        <img src="images/images-print.png" alt="Print" width="25"></a>
   </div>
          </aside><nav><div class="header">Di&#7877;n &#273;&agrave;n</div>
            <a href="https://forum.machinelearningcoban.com">
            <img width="100%" src="images/latex-new_logo9-2.png"></a>
          </nav><nav><div class="header">Interactive Learning</div>
            <a href="https://fundaml.com">
            <img width="100%" src="images/images-fundaml_web.png"></a>
          </nav><nav><div class="header" with="100%">Facebook page</div>
          <!-- <a href = "https://www.facebook.com/machinelearningbasicvn/" target="_blank" title="Follow us"><img src = "/assets/images/facebook.png" width="30"></a> -->
          <!-- facebook page -->

         <div class="fb-page" data-href="https://www.facebook.com/machinelearningbasicvn/" data-width="250" data-small-header="false" data-adapt-container-width="true" data-hide-cover="false" data-show-facepile="false"><blockquote cite="https://www.facebook.com/machinelearningbasicvn/" class="fb-xfbml-parse-ignore"><a style="color: #204081" href="https://www.facebook.com/machinelearningbasicvn/">Machine Learning c&#417; b&#7843;n</a></blockquote></div>
          <!--end facebook page -->

          </nav><nav><div class="header">Facebook group</div>
            <a href="https://www.facebook.com/groups/257768141347267/">
            <img width="100%" src="images/14_mlp-multi_layers.png"></a>
          </nav><nav><div class="header">Recommended books</div>
            <ul><li> <a style="text-align: left; color: #074B80;" href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwjd7Y_Q-tzTAhVISyYKHUXyCekQFggvMAA&amp;url=http%3A%2F%2Fusers.isr.ist.utl.pt%2F~wurmd%2FLivros%2Fschool%2FBishop%2520-%2520Pattern%2520Recognition%2520And%2520Machine%2520Learning%2520-%2520Springer%2520%25202006.pdf&amp;usg=AFQjCNEVQzQ_dEpxG4P7NamTWUXnVXzCng&amp;sig2=H1WVtom4rq3uh8UfbGX4oA">"Pattern recognition and Machine Learning.", C. Bishop </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://github.com/tpn/pdfs/blob/master/The%20Elements%20of%20Statistical%20Learning%20-%20Data%20Mining%2C%20Inference%20and%20Prediction%20-%202nd%20Edition%20(ESLII_print4).pdf">"The Elements of Statistical Learning", T. Hastie et al.  </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://www.computervisionmodels.com/">"Computer Vision:  Models, Learning, and Inference", Simon J.D. Prince </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://stanford.edu/~boyd/cvxbook/">"Convex Optimization", Boyd and Vandenberghe</a></li>

            </ul></nav><nav><div class="header">Recommended courses</div>

          <ul><li> <a style="text-align: left; color: #074B80;" href="https://www.coursera.org/learn/machine-learning?utm_source=gg&amp;utm_medium=sem&amp;campaignid=693373197&amp;adgroupid=36745103515&amp;device=c&amp;keyword=machine%20learning%20andrew%20ng&amp;matchtype=e&amp;network=g&amp;devicemodel=&amp;adpostion=1t1&amp;creativeid=156061453588&amp;hide_mobile_promo&amp;gclid=Cj0KEQjwt6fHBRDtm9O8xPPHq4gBEiQAdxotvNEC6uHwKB5Ik_W87b9mo-zTkmj9ietB4sI8-WWmc5UaAi6a8P8HAQ">"Machine Learning", Andrew Ng </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs224n/">CS224n: Natural Language Processing with Deep Learning</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>           

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs246/">CS246: Mining Massive Data Sets</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://web.stanford.edu/class/cs20si/syllabus.html">CS20SI: Tensorflow for Deep Learning Research </a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-10">Introduction to Computer Science and Programming Using Python</a></li>           

            </ul></nav><nav><div class="header">Others</div>
          <ul><li> <a style="text-align: left; color: #074B80;" href="https://github.com/ZuzooVn/machine-learning-for-software-engineers">Top-down learning path: Machine Learning for Software Engineers</a></li>
              
              <li> <a style="text-align: left; color: #074B80;" href="howdoIcreatethisblog.html">Blog n&agrave;y &#273;&#432;&#7907;c t&#7841;o nh&#432; th&#7871; n&agrave;o?</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://thepresentwriter.com/chung-toi-da-apply-va-hoc-tien-si-nhu-the-nao-phan-1/">Ch&uacute;ng t&ocirc;i &#273;&atilde; apply v&agrave; h&#7885;c ti&#7871;n s&#7929; nh&#432; th&#7871; n&agrave;o? (1/2)</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://thepresentwriter.com/chung-toi-da-apply-va-hoc-tien-si-nhu-the-nao-phan-2/">Ch&uacute;ng t&ocirc;i &#273;&atilde; apply v&agrave; h&#7885;c ti&#7871;n s&#7929; nh&#432; th&#7871; n&agrave;o? (2/2)</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="http://machinelearningmastery.com/inspirational-applications-deep-learning/">8 Inspirational Applications of Deep Learning</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://ccrma.stanford.edu/~dattorro/matrixcalc.pdf">Matrix calculus</a></li>

              <li> <a style="text-align: left; color: #074B80;" href="https://github.com/aymericdamien/TensorFlow-Examples">TensorFlow-Examples</a></li>
              
              <li> <a style="text-align: left; color: #074B80;" href="https://www.forbes.com/sites/quora/2017/04/05/eight-easy-steps-to-get-started-learning-artificial-intelligence/#53c29fa5b117">Eight Easy Steps To Get Started Learning Artificial Intelligence</a></li>
              <li> <a style="text-align: left; color: #074B80;" href="https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The 9 Deep Learning Papers You Need To Know About</a></li>

                     

            </ul></nav><!-- <img style = "transform: scaleX(1); width:100%; margin-left:00px;position: absolute;" src = "/images/mai.jpg"> --><!--   
            <nav>
              <div class="header">Previous by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #074B80;" href="/2017/01/12/gradientdescent/">B&agrave;i 7: Gradient Descent (ph&#7847;n 1/2)</a></li>
              </ul>
            </nav>
           
           
            <nav>
              <div class="header">Next by date</div>
              <ul>
                <li><a style="text-align: left; font-family: 'Roboto Condensed', sans-serif; color: #204081;" href="/2017/01/21/perceptron/">B&agrave;i 9: Perceptron Learning Algorithm</a></li>
              </ul>
            </nav>
            --><!-- <img style = "transform: scaleX(1); width:250%; margin-left:-100px;" src = "/images/dao.jpg"> --><!-- <a href ="https://www.facebook.com/masspvn/?fref=nf&pnref=story">MaSSP</a> --></div>
      	</div>
    </div>
<script data-cfasync="false" src="js/cloudflare-static-email-decode.min.js"></script></body></html>
